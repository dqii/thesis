{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dqi/.conda/envs/py36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/home/dqi/.conda/envs/py36/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from features.settings import Baseline_Settings, Pair_Hinge_Settings, Pair_Log_Settings, Triplet_Hinge_Settings, Triplet_Log_Settings\n",
    "from features.models import ConvModelSmall, ConvModelMedium, ConvModelLarge\n",
    "from features.models import GCNNModelSmall, GCNNModelMedium, GCNNModelLarge\n",
    "from classes.models import LinearClassifier\n",
    "from datasets import load_cifar100\n",
    "from sklearn.svm import LinearSVC\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR-100 loaded in 103 seconds\n"
     ]
    }
   ],
   "source": [
    "data, samplers, pair_samplers, triplet_samplers = load_cifar100(augment=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_model(feature_model, settings, data, num_features, num_classes, feature_samplers, classification_samplers,\n",
    "               f_num_steps, f_lr, f_keep_prob, f_reg, c_num_steps, c_lr, c_reg):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    f_train, f_valid, f_test = feature_samplers\n",
    "    c_train, c_valid, c_test = classification_samplers\n",
    "    (x_train, y_train), (x_valid, y_valid), (x_test, y_test) = data\n",
    "    _, H, W, C = x_train.shape\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, shape=[None, H, W, C])\n",
    "    y = tf.placeholder(tf.int64, shape=[None])    \n",
    "    dropout = tf.placeholder(tf.float32)\n",
    "    training = tf.placeholder(tf.bool)\n",
    "    f_model = feature_model(x=x, y=y, settings=settings, num_chan=C, num_features=num_features, \n",
    "              lr=f_lr, reg=f_reg, dropout=dropout, training=training)\n",
    "    \n",
    "    features = tf.placeholder(tf.float32, shape=[None, num_features])\n",
    "    c_model = LinearClassifier(x=features, y=y, num_features=num_features, num_classes=num_features, \n",
    "                               lr=c_lr, reg=c_reg)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        print(\"begin training features // num_features: %g, lr: %g, reg: %g, dropout: %g\" %(num_features, f_lr, f_reg, f_keep_prob))\n",
    "        train_time = time.time()\n",
    "        for step in range(f_num_steps):        \n",
    "            x_, y_ = f_train.sample(700)\n",
    "            sess.run(f_model.optimize, feed_dict={x:x_, y:y_, dropout:f_keep_prob, training:True})     \n",
    "            if step % 1000 == 0:\n",
    "                train_loss, train_acc = sess.run([f_model.loss, f_model.acc], \n",
    "                                                 feed_dict={x:x_, y:y_, dropout:1.0, training:False})\n",
    "                print(\"\\tstep %d: train loss %g, train error %g\"%(step, train_loss, 1 - train_acc))  \n",
    "        train_time = time.time() - train_time\n",
    "        print(\"end training features // time elapsed: %.4f s\"%(train_time))\n",
    "        \n",
    "        eval_test_time = time.time()\n",
    "        x_, y_ = f_valid.sample(1000)\n",
    "        test_error = 1 - sess.run(f_model.acc, feed_dict={x:x_, y:y_, dropout:1.0, training:False})\n",
    "        eval_test_time = time.time() - eval_test_time\n",
    "        print(\"validation set error: %.4f // time elapsed: %.4f s\"%(test_error, eval_test_time))\n",
    "        \n",
    "        eval_test_time = time.time()\n",
    "        x_, y_ = f_test.sample(1000)\n",
    "        test_error = 1 - sess.run(f_model.acc, feed_dict={x:x_, y:y_, dropout:1.0, training:False})\n",
    "        eval_test_time = time.time() - eval_test_time\n",
    "        print(\"test set error: %.4f // time elapsed: %.4f s\"%(test_error, eval_test_time))\n",
    "        \n",
    "        print(\"begin training classifier\")\n",
    "        train_time = time.time()\n",
    "        for step in range(c_num_steps):        \n",
    "            x_, y_ = c_train.sample(700)\n",
    "            features_ = sess.run(f_model.features, feed_dict={x:x_, dropout:1.0, training:False})\n",
    "            sess.run(c_model.optimize, feed_dict={features:features_, y:y_, x:x_, dropout:1.0, training:False})\n",
    "            if step % 1000 == 0:\n",
    "                train_loss, train_acc = sess.run([c_model.loss, c_model.acc], feed_dict={features:features_, y:y_})\n",
    "                print(\"\\tstep %d: train loss %g, train error %g\"%(step, train_loss, 1 - train_acc))  \n",
    "        train_time = time.time() - train_time\n",
    "        print(\"end training classifier // time elapsed: %.4f s\"%(train_time))\n",
    "        \n",
    "        eval_test_time = time.time()\n",
    "        features_ = sess.run(f_model.features, feed_dict={x:x_valid, dropout:1.0, training:False})\n",
    "        test_error = 1 - sess.run(c_model.acc, feed_dict={features:features_, y:y_valid})\n",
    "        eval_test_time = time.time() - eval_test_time\n",
    "        print(\"validation set error: %.4f // time elapsed: %.4f s\"%(test_error, eval_test_time))\n",
    "        \n",
    "        eval_test_time = time.time()\n",
    "        features_ = sess.run(f_model.features, feed_dict={x:x_test, dropout:1.0, training:False})\n",
    "        test_error = 1 - sess.run(c_model.acc, feed_dict={features:features_, y:y_test})\n",
    "        eval_test_time = time.time() - eval_test_time\n",
    "        print(\"test set error: %.4f // time elapsed: %.4f s\"%(test_error, eval_test_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin training features // num_features: 100, lr: 0.001, reg: 0.001, dropout: 0.7\n",
      "\tstep 0: train loss 5.17647, train error 0.997143\n",
      "\tstep 1000: train loss 5.00519, train error 0.957143\n",
      "\tstep 2000: train loss 4.95009, train error 0.961429\n",
      "\tstep 3000: train loss 4.90154, train error 0.941429\n",
      "\tstep 4000: train loss 4.80595, train error 0.935714\n",
      "\tstep 5000: train loss 4.80295, train error 0.94\n",
      "\tstep 6000: train loss 4.66479, train error 0.897143\n",
      "\tstep 7000: train loss 4.6549, train error 0.912857\n",
      "\tstep 8000: train loss 4.55748, train error 0.907143\n",
      "\tstep 9000: train loss 4.56257, train error 0.9\n",
      "\tstep 10000: train loss 4.55824, train error 0.901429\n",
      "\tstep 11000: train loss 4.51331, train error 0.908571\n",
      "\tstep 12000: train loss 4.48371, train error 0.885714\n",
      "\tstep 13000: train loss 4.40715, train error 0.885714\n",
      "\tstep 14000: train loss 4.37365, train error 0.861429\n",
      "\tstep 15000: train loss 4.38407, train error 0.864286\n",
      "\tstep 16000: train loss 4.36995, train error 0.867143\n",
      "\tstep 17000: train loss 4.31793, train error 0.857143\n",
      "\tstep 18000: train loss 4.26897, train error 0.844286\n",
      "\tstep 19000: train loss 4.3522, train error 0.86\n",
      "\tstep 20000: train loss 4.2664, train error 0.851429\n",
      "\tstep 21000: train loss 4.24937, train error 0.847143\n",
      "\tstep 22000: train loss 4.19766, train error 0.834286\n",
      "\tstep 23000: train loss 4.14263, train error 0.845714\n",
      "\tstep 24000: train loss 4.11352, train error 0.814286\n",
      "\tstep 25000: train loss 4.1375, train error 0.855714\n",
      "\tstep 26000: train loss 4.17768, train error 0.838571\n",
      "\tstep 27000: train loss 4.1155, train error 0.838571\n",
      "\tstep 28000: train loss 4.07947, train error 0.821429\n",
      "\tstep 29000: train loss 4.03196, train error 0.815714\n",
      "\tstep 30000: train loss 3.9884, train error 0.828571\n",
      "\tstep 31000: train loss 3.97621, train error 0.804286\n",
      "\tstep 32000: train loss 4.02384, train error 0.848571\n",
      "\tstep 33000: train loss 3.99269, train error 0.828571\n",
      "\tstep 34000: train loss 3.92321, train error 0.808571\n",
      "\tstep 35000: train loss 3.8569, train error 0.781429\n",
      "\tstep 36000: train loss 3.9772, train error 0.808571\n",
      "\tstep 37000: train loss 3.90628, train error 0.798571\n",
      "\tstep 38000: train loss 3.97533, train error 0.828571\n",
      "\tstep 39000: train loss 3.86828, train error 0.802857\n",
      "\tstep 40000: train loss 3.80853, train error 0.784286\n",
      "\tstep 41000: train loss 3.81861, train error 0.792857\n",
      "\tstep 42000: train loss 3.80958, train error 0.782857\n",
      "\tstep 43000: train loss 3.83982, train error 0.811429\n",
      "\tstep 44000: train loss 3.7456, train error 0.781429\n",
      "\tstep 45000: train loss 3.78403, train error 0.8\n",
      "\tstep 46000: train loss 3.68265, train error 0.754286\n",
      "\tstep 47000: train loss 3.75854, train error 0.788571\n",
      "\tstep 48000: train loss 3.78459, train error 0.782857\n",
      "\tstep 49000: train loss 3.73649, train error 0.768571\n",
      "end training features // time elapsed: 2709.6803 s\n",
      "test set error: 0.7710 // time elapsed: 0.0758 s\n",
      "begin training classifier\n",
      "\tstep 0: train loss 6.14156, train error 0.995714\n",
      "\tstep 1000: train loss 4.15715, train error 0.88\n",
      "\tstep 2000: train loss 3.89659, train error 0.841429\n",
      "\tstep 3000: train loss 3.79619, train error 0.831429\n",
      "\tstep 4000: train loss 3.79352, train error 0.825714\n",
      "\tstep 5000: train loss 3.70145, train error 0.788571\n",
      "\tstep 6000: train loss 3.54117, train error 0.778571\n",
      "\tstep 7000: train loss 3.54964, train error 0.775714\n",
      "\tstep 8000: train loss 3.54424, train error 0.788571\n",
      "\tstep 9000: train loss 3.40381, train error 0.755714\n",
      "\tstep 10000: train loss 3.47239, train error 0.781429\n",
      "\tstep 11000: train loss 3.3895, train error 0.762857\n",
      "\tstep 12000: train loss 3.39202, train error 0.752857\n",
      "\tstep 13000: train loss 3.46554, train error 0.794286\n",
      "\tstep 14000: train loss 3.385, train error 0.78\n",
      "\tstep 15000: train loss 3.48717, train error 0.8\n",
      "\tstep 16000: train loss 3.40388, train error 0.781429\n",
      "\tstep 17000: train loss 3.41247, train error 0.778571\n",
      "\tstep 18000: train loss 3.39233, train error 0.778571\n",
      "\tstep 19000: train loss 3.37007, train error 0.784286\n",
      "\tstep 20000: train loss 3.31471, train error 0.755714\n",
      "\tstep 21000: train loss 3.37065, train error 0.78\n",
      "\tstep 22000: train loss 3.20595, train error 0.732857\n",
      "\tstep 23000: train loss 3.31921, train error 0.785714\n",
      "\tstep 24000: train loss 3.26387, train error 0.767143\n",
      "\tstep 25000: train loss 3.32897, train error 0.765714\n",
      "\tstep 26000: train loss 3.31377, train error 0.778571\n",
      "\tstep 27000: train loss 3.29586, train error 0.768571\n",
      "\tstep 28000: train loss 3.25025, train error 0.738571\n",
      "\tstep 29000: train loss 3.2493, train error 0.744\n",
      "end training classifier // time elapsed: 1277.5730 s\n",
      "validation set error: 0.7645 // time elapsed: 0.3212 s\n",
      "test set error: 0.7577 // time elapsed: 0.5060 s\n"
     ]
    }
   ],
   "source": [
    "test_model(feature_model=GCNNModelSmall, settings=Baseline_Settings, num_features=100, num_classes=100,\n",
    "           data=data, feature_samplers=samplers, classification_samplers=samplers,           \n",
    "           f_num_steps=70000, f_lr=0.005, f_keep_prob=0.6, f_reg=0.001, \n",
    "           c_num_steps=30000, c_lr=0.005, c_reg=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin training features // num_features: 100, lr: 0.001, reg: 0.001, dropout: 0.7\n",
      "\tstep 0: train loss 1.53548, train error 0.478571\n",
      "\tstep 1000: train loss 1.47141, train error 0.452857\n",
      "\tstep 2000: train loss 1.44291, train error 0.398571\n",
      "\tstep 3000: train loss 1.48774, train error 0.452857\n",
      "\tstep 4000: train loss 1.48073, train error 0.45\n",
      "\tstep 5000: train loss 1.45626, train error 0.447143\n",
      "\tstep 6000: train loss 1.48187, train error 0.434286\n",
      "\tstep 7000: train loss 1.45791, train error 0.427143\n",
      "\tstep 8000: train loss 1.46465, train error 0.46\n",
      "\tstep 9000: train loss 1.44453, train error 0.431429\n",
      "\tstep 10000: train loss 1.38156, train error 0.411429\n",
      "\tstep 11000: train loss 1.348, train error 0.41\n",
      "\tstep 12000: train loss 1.38945, train error 0.42\n",
      "\tstep 13000: train loss 1.37956, train error 0.412857\n",
      "\tstep 14000: train loss 1.37284, train error 0.407143\n",
      "\tstep 15000: train loss 1.35174, train error 0.384286\n",
      "\tstep 16000: train loss 1.39454, train error 0.402857\n",
      "\tstep 17000: train loss 1.3183, train error 0.341429\n",
      "\tstep 18000: train loss 1.36114, train error 0.405714\n",
      "\tstep 19000: train loss 1.29577, train error 0.352857\n",
      "\tstep 20000: train loss 1.32253, train error 0.38\n",
      "\tstep 21000: train loss 1.33206, train error 0.387143\n",
      "\tstep 22000: train loss 1.29921, train error 0.372857\n",
      "\tstep 23000: train loss 1.26955, train error 0.361429\n",
      "\tstep 24000: train loss 1.26361, train error 0.351429\n",
      "\tstep 25000: train loss 1.26937, train error 0.365714\n",
      "\tstep 26000: train loss 1.26251, train error 0.365714\n",
      "\tstep 27000: train loss 1.20225, train error 0.32\n",
      "\tstep 28000: train loss 1.2236, train error 0.33\n",
      "\tstep 29000: train loss 1.24038, train error 0.388571\n",
      "\tstep 30000: train loss 1.24992, train error 0.342857\n",
      "\tstep 31000: train loss 1.21835, train error 0.335714\n",
      "\tstep 32000: train loss 1.25081, train error 0.37\n",
      "\tstep 33000: train loss 1.20445, train error 0.337143\n",
      "\tstep 34000: train loss 1.18156, train error 0.34\n",
      "\tstep 35000: train loss 1.23834, train error 0.362857\n",
      "\tstep 36000: train loss 1.18554, train error 0.318571\n",
      "\tstep 37000: train loss 1.20395, train error 0.34\n",
      "\tstep 38000: train loss 1.1756, train error 0.341429\n",
      "\tstep 39000: train loss 1.18342, train error 0.332857\n",
      "\tstep 40000: train loss 1.21861, train error 0.36\n",
      "\tstep 41000: train loss 1.16393, train error 0.312857\n",
      "\tstep 42000: train loss 1.18319, train error 0.322857\n",
      "\tstep 43000: train loss 1.15386, train error 0.328571\n",
      "\tstep 44000: train loss 1.20287, train error 0.348571\n",
      "\tstep 45000: train loss 1.18453, train error 0.348571\n",
      "\tstep 46000: train loss 1.18852, train error 0.327143\n",
      "\tstep 47000: train loss 1.2288, train error 0.367143\n",
      "\tstep 48000: train loss 1.14384, train error 0.315714\n",
      "\tstep 49000: train loss 1.18883, train error 0.354286\n",
      "end training features // time elapsed: 8375.3102 s\n",
      "test set error: 0.3460 // time elapsed: 0.2113 s\n",
      "begin training classifier\n",
      "\tstep 0: train loss 5.11136, train error 0.987143\n",
      "\tstep 1000: train loss 5.05209, train error 0.97\n",
      "\tstep 2000: train loss 4.98196, train error 0.982857\n",
      "\tstep 3000: train loss 4.92888, train error 0.961429\n",
      "\tstep 4000: train loss 4.85663, train error 0.967143\n",
      "\tstep 5000: train loss 4.82895, train error 0.961429\n",
      "\tstep 6000: train loss 4.79231, train error 0.968571\n",
      "\tstep 7000: train loss 4.7327, train error 0.954286\n",
      "\tstep 8000: train loss 4.69556, train error 0.955714\n",
      "\tstep 9000: train loss 4.655, train error 0.942857\n",
      "\tstep 10000: train loss 4.64115, train error 0.94\n",
      "\tstep 11000: train loss 4.62142, train error 0.934286\n",
      "\tstep 12000: train loss 4.59383, train error 0.938571\n",
      "\tstep 13000: train loss 4.54674, train error 0.935714\n",
      "\tstep 14000: train loss 4.53285, train error 0.917143\n",
      "\tstep 15000: train loss 4.49725, train error 0.911429\n",
      "\tstep 16000: train loss 4.5433, train error 0.954286\n",
      "\tstep 17000: train loss 4.5026, train error 0.94\n",
      "\tstep 18000: train loss 4.47699, train error 0.938571\n",
      "\tstep 19000: train loss 4.44505, train error 0.931429\n",
      "\tstep 20000: train loss 4.46489, train error 0.934286\n",
      "\tstep 21000: train loss 4.43449, train error 0.925714\n",
      "\tstep 22000: train loss 4.416, train error 0.942857\n",
      "\tstep 23000: train loss 4.41431, train error 0.944286\n",
      "\tstep 24000: train loss 4.39302, train error 0.922857\n",
      "\tstep 25000: train loss 4.40966, train error 0.941429\n",
      "\tstep 26000: train loss 4.33713, train error 0.928571\n",
      "\tstep 27000: train loss 4.38232, train error 0.952857\n",
      "\tstep 28000: train loss 4.34502, train error 0.928571\n",
      "\tstep 29000: train loss 4.37649, train error 0.932857\n",
      "end training classifier // time elapsed: 1249.0459 s\n",
      "validation set error: 0.9365 // time elapsed: 0.2317 s\n",
      "test set error: 0.9335 // time elapsed: 0.3508 s\n"
     ]
    }
   ],
   "source": [
    "test_model(feature_model=GCNNModelSmall, settings=Triplet_Hinge_Settings, num_features=100, num_classes=100,\n",
    "           data=data, feature_samplers=triplet_samplers, classification_samplers=samplers,           \n",
    "           f_num_steps=70000, f_lr=0.005, f_keep_prob=0.6, f_reg=0.001, \n",
    "           c_num_steps=30000, c_lr=0.005, c_reg=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_model(feature_model=GCNNModelSmall, settings=Triplet_Hinge_Settings, num_features=100, num_classes=100,\n",
    "           data=data, feature_samplers=triplet_samplers, classification_samplers=samplers,           \n",
    "           f_num_steps=70000, f_lr=0.005, f_keep_prob=0.6, f_reg=0.001, \n",
    "           c_num_steps=30000, c_lr=0.005, c_reg=0.01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
