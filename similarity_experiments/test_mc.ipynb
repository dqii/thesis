{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from features.settings import Baseline_Settings, Triplet_Hinge_Settings, Triplet_Log_Settings\n",
    "from features.models import ConvModelSmall, ConvModelMedium, ConvModelLarge\n",
    "from features.models import GCNNModelSmall, GCNNModelMedium, GCNNModelLarge\n",
    "from classes.models import LinearClassifier0, LinearClassifier1, LinearClassifier2, LinearClassifier3, TwoLayerClassifier\n",
    "from learn import get_feature_model, get_classifier, train_features, train_classifier, test_model\n",
    "from datasets import load_mnist_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST loaded in 18 seconds\n",
      "Fashion MNIST loaded in 36 seconds\n"
     ]
    }
   ],
   "source": [
    "data, samplers, pair_samplers, triplet_samplers = load_mnist_datasets(augment=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "f, f_params = get_feature_model(feature_model=ConvModelSmall, settings=Triplet_Hinge_Settings,\n",
    "                                num_features=10, num_classes=10, data_format=[28, 28, 1],\n",
    "                                f_lr=0.07, f_reg=0.001, f_scope=\"f_cms_mnist_10\")\n",
    "fs = tf.train.Saver(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, \"f_cms_mnist_10\"))\n",
    "d = data['mnist']; s=triplet_samplers['mnist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/f_cms_mnist_10.ckpt\n",
      "begin training classifier\n",
      "\tstep 0: train loss 2.33491, train error 0.911429\n",
      "\tstep 100: train loss 2.2456, train error 0.893333\n",
      "\tstep 200: train loss 2.20971, train error 0.683333\n",
      "\tstep 300: train loss 2.19599, train error 0.709524\n",
      "\tstep 400: train loss 2.17701, train error 0.697143\n",
      "\tstep 500: train loss 2.20902, train error 0.72619\n",
      "\tstep 600: train loss 2.15796, train error 0.697143\n",
      "\tstep 700: train loss 2.16784, train error 0.709048\n",
      "\tstep 800: train loss 2.14705, train error 0.699048\n",
      "\tstep 900: train loss 2.17079, train error 0.712381\n",
      "\tstep 1000: train loss 2.15888, train error 0.708095\n",
      "\tstep 1100: train loss 2.17433, train error 0.719048\n",
      "\tstep 1200: train loss 2.18846, train error 0.728095\n",
      "\tstep 1300: train loss 2.15536, train error 0.70381\n",
      "\tstep 1400: train loss 2.18539, train error 0.73381\n",
      "\tstep 1500: train loss 2.15881, train error 0.709048\n",
      "\tstep 1600: train loss 2.15959, train error 0.710952\n",
      "\tstep 1700: train loss 2.13224, train error 0.691905\n",
      "\tstep 1800: train loss 2.14283, train error 0.698095\n",
      "\tstep 1900: train loss 2.17081, train error 0.719524\n",
      "\tstep 2000: train loss 2.16035, train error 0.711429\n",
      "\tstep 2100: train loss 2.16043, train error 0.714286\n",
      "\tstep 2200: train loss 2.16161, train error 0.71381\n",
      "\tstep 2300: train loss 2.13338, train error 0.693333\n",
      "\tstep 2400: train loss 2.15525, train error 0.710952\n",
      "\tstep 2500: train loss 2.15253, train error 0.708095\n",
      "\tstep 2600: train loss 2.14226, train error 0.699524\n",
      "\tstep 2700: train loss 2.14523, train error 0.701429\n",
      "\tstep 2800: train loss 2.13751, train error 0.69619\n",
      "\tstep 2900: train loss 2.12348, train error 0.688095\n",
      "\tstep 3000: train loss 2.13612, train error 0.698095\n",
      "\tstep 3100: train loss 2.15769, train error 0.71381\n",
      "\tstep 3200: train loss 2.14511, train error 0.70381\n",
      "\tstep 3300: train loss 2.14239, train error 0.701905\n",
      "\tstep 3400: train loss 2.13339, train error 0.695714\n",
      "\tstep 3500: train loss 2.13642, train error 0.696667\n",
      "\tstep 3600: train loss 2.14609, train error 0.70619\n",
      "\tstep 3700: train loss 2.14584, train error 0.70381\n",
      "\tstep 3800: train loss 2.14636, train error 0.705238\n",
      "\tstep 3900: train loss 2.15739, train error 0.715714\n",
      "\tstep 4000: train loss 2.14176, train error 0.702857\n",
      "\tstep 4100: train loss 2.12918, train error 0.692857\n",
      "\tstep 4200: train loss 2.12367, train error 0.687143\n",
      "\tstep 4300: train loss 2.14572, train error 0.705714\n",
      "\tstep 4400: train loss 2.13991, train error 0.701905\n",
      "\tstep 4500: train loss 2.14495, train error 0.704286\n",
      "\tstep 4600: train loss 2.12597, train error 0.690476\n",
      "\tstep 4700: train loss 2.1415, train error 0.704762\n",
      "\tstep 4800: train loss 2.11771, train error 0.687619\n",
      "\tstep 4900: train loss 2.17073, train error 0.72619\n",
      "end training classifier // time elapsed: 254.3043 s\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_error' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-4bb5189dd7cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./models/f_cms_mnist_10.ckpt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     train_classifier(sess=sess, feature_model=f, classifier=c, data=d, samplers=s, num_steps=5000, \n\u001b[0;32m----> 9\u001b[0;31m                      f_params=f_params, c_params=c_params)\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/thesis/similarity_experiments/learn.py\u001b[0m in \u001b[0;36mtrain_classifier\u001b[0;34m(sess, feature_model, classifier, data, samplers, num_steps, f_params, c_params)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mvalid_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfeatures_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0meval_test_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0meval_test_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"validation set error: %.4f // time elapsed: %.4f s\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_test_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_error' is not defined"
     ]
    }
   ],
   "source": [
    "c, c_params = get_classifier(classifier=LinearClassifier0, num_features=10, num_classes=10, \n",
    "                             c_lr=0.005, c_reg=0.001, c_scope=\"boosting\", f_params=f_params)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    fs.restore(sess, \"./models/f_cms_mnist_10.ckpt\")\n",
    "    train_classifier(sess=sess, feature_model=f, classifier=c, data=d, samplers=s, num_steps=5000, \n",
    "                     f_params=f_params, c_params=c_params)\n",
    "    test_model(sess=sess, feature_model=f, classifier=c, samplers=s, data=d, f_params=f_params, c_params=c_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/f_cms_mnist_10.ckpt\n",
      "begin training features\n",
      "\tstep 0: train loss 1.18365, train error 0.424286\n",
      "\tstep 10: train loss 1.18504, train error 0.445714\n",
      "\tstep 20: train loss 1.182, train error 0.438571\n",
      "\tstep 30: train loss 1.18222, train error 0.478571\n",
      "\tstep 40: train loss 1.17933, train error 0.44\n",
      "\tstep 50: train loss 1.18055, train error 0.458571\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-8c3e9527feb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./models/f_cms_mnist_10.ckpt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrain_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     train_classifier(sess=sess, feature_model=f, classifier=c, data=d, samplers=s, num_steps=1000, \n\u001b[1;32m     10\u001b[0m                      f_params=f_params, c_params=c_params)\n",
      "\u001b[0;32m~/thesis/similarity_experiments/learn.py\u001b[0m in \u001b[0;36mtrain_features\u001b[0;34m(sess, feature_model, samplers, num_steps, keep_prob, f_params)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mx_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m700\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             train_loss, train_acc = sess.run([feature_model.loss, feature_model.acc], \n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "c, c_params = get_classifier(classifier=LinearClassifier1, num_features=10, num_classes=10, \n",
    "                             c_lr=0.02, c_reg=0.001, c_scope=\"boosting\", f_params=f_params)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    fs.restore(sess, \"./models/f_cms_mnist_10.ckpt\")\n",
    "    train_classifier(sess=sess, feature_model=f, classifier=c, data=d, samplers=s, num_steps=1000, \n",
    "                     f_params=f_params, c_params=c_params)\n",
    "    test_model(sess=sess, feature_model=f, classifier=c, samplers=s, data=d, f_params=f_params, c_params=c_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c, c_params = get_classifier(classifier=LinearClassifier2, num_features=10, num_classes=10, \n",
    "                             c_lr=0.02, c_reg=0.001, c_scope=\"boosting\", f_params=f_params)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    fs.restore(sess, \"./models/f_cms_mnist_10.ckpt\")\n",
    "    train_classifier(sess=sess, feature_model=f, classifier=c, data=d, samplers=s, num_steps=10000, \n",
    "                     f_params=f_params, c_params=c_params)\n",
    "    test_model(sess=sess, feature_model=f, classifier=c, samplers=s, data=d, f_params=f_params, c_params=c_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c, c_params = get_classifier(classifier=LinearClassifier3, num_features=10, num_classes=10, \n",
    "                             c_lr=0.02, c_reg=0.001, c_scope=\"boosting\", f_params=f_params)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    fs.restore(sess, \"./models/f_cms_mnist_10.ckpt\")\n",
    "    train_classifier(sess=sess, feature_model=f, classifier=c, data=d, samplers=s, num_steps=10000, \n",
    "                     f_params=f_params, c_params=c_params)\n",
    "    test_model(sess=sess, feature_model=f, classifier=c, samplers=s, data=d, f_params=f_params, c_params=c_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.array([[1,2,3,4],[1,7,3,5],[2,4,1,1],[1,3,4,4],[4,4,4,4]])\n",
    "labels = np.array([0,1,1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.68972664, 0.61836901, 0.69221705, 1.55144471])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.array([0,1,1,2])\n",
    "a = np.array([[0.8, 0.1, 0.1], [0.1,0.9,0], [0.2,0.8,0],[0,1,0]])\n",
    "tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=a).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = np.zeros((5, 4))\n",
    "b[np.arange(5), labels] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels1 = b[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6186249239125281\n",
      "0.61862475\n",
      "0.8958837961365061\n",
      "0.89588344\n",
      "0.27970577975764666\n",
      "0.27970564\n",
      "0.8886102673023313\n",
      "0.8886099\n"
     ]
    }
   ],
   "source": [
    "for i in range():\n",
    "    labels = b[:,i]\n",
    "    print(np.mean(-(labels * np.log(predictions) + (1-labels) * np.log(1-predictions))))\n",
    "    print(tf.losses.log_loss(labels, predictions).eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.61862492 0.8958838  0.27970578 0.88861027]\n",
      "0.6707059\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(-(labels * np.log(predictions) + (1-labels) * np.log(1-predictions))))\n",
    "print(tf.losses.log_loss(labels, predictions).eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = np.array([0.3, 0.3, 0.2, 0.7, 0.1]).reshape((5,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = np.repeat(predictions, 4, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6186249  0.89588386 0.2797058  0.88861024]\n"
     ]
    }
   ],
   "source": [
    "B = tf.convert_to_tensor(b.T, np.float32)\n",
    "P = tf.convert_to_tensor(p.T, np.float32)\n",
    "print(losses.binary_crossentropy(B, P).eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3, 0.3, 0.2, 0.7, 0.1],\n",
       "       [0.3, 0.3, 0.2, 0.7, 0.1],\n",
       "       [0.3, 0.3, 0.2, 0.7, 0.1],\n",
       "       [0.3, 0.3, 0.2, 0.7, 0.1]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.array([[[ 1,  2,  3],\n",
    "  [ 4,  5,  6]],\n",
    " [[ 7,  8,  9],\n",
    "  [10, 11, 12]]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
