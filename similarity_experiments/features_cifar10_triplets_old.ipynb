{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from features.settings import Baseline_Settings, Triplet_Hinge_Settings, Triplet_Log_Settings, Pair_Hinge_Settings\n",
    "from features.models import GCNNSmall0, GCNNMedium0, GCNNMedium1, GCNNLarge0\n",
    "from learn import get_feature_model, train_features, test_features\n",
    "from datasets import load_cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR-10 loaded in 104 seconds\n"
     ]
    }
   ],
   "source": [
    "data, samplers, pair_samplers, triplet_samplers = load_cifar10(augment=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d=data; s=triplet_samplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/cifar10/f_gm1_cifar10_10.ckpt\n",
      "test set error: 0.0565 // time elapsed: 315.7220 s\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "f, f_params = get_feature_model(feature_model=GCNNMedium1, settings=Triplet_Hinge_Settings,\n",
    "                                num_features=10, data_format=[32, 32, 3],\n",
    "                                f_lr=0.04, f_reg=0.001, f_scope=\"f_gm1_cifar10_10\")\n",
    "fs = tf.train.Saver(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, \"f_gm1_cifar10_10\"))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    fs.restore(sess, \"./models/cifar10/f_gm1_cifar10_10.ckpt\")\n",
    "    test_features(sess=sess, feature_model=f, samplers=s, f_params=f_params)\n",
    "#     train_features(sess=sess, feature_model=f, samplers=s, num_steps=40000, keep_prob=0.7, f_params=f_params)\n",
    "#     fs.save(sess, \"./models/cifar10/f_gm1_cifar10_10.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin training features\n",
      "\tstep 0: train loss 8.5841, train error 0.463333\n",
      "\tstep 200: train loss 1.44429, train error 0.483333\n",
      "\tstep 400: train loss 1.34448, train error 0.436667\n",
      "\tstep 600: train loss 1.08126, train error 0.26\n",
      "\tstep 800: train loss 1.02227, train error 0.273333\n",
      "\tstep 1000: train loss 0.955222, train error 0.25\n",
      "\tstep 1200: train loss 0.968754, train error 0.266667\n",
      "\tstep 1400: train loss 0.995098, train error 0.276667\n",
      "\tstep 1600: train loss 0.971295, train error 0.233333\n",
      "\tstep 1800: train loss 1.18578, train error 0.376667\n",
      "\tstep 2000: train loss 0.789963, train error 0.213333\n",
      "\tstep 2200: train loss 0.735144, train error 0.186667\n",
      "\tstep 2400: train loss 0.689266, train error 0.176667\n",
      "\tstep 2600: train loss 0.747502, train error 0.213333\n",
      "\tstep 2800: train loss 0.692215, train error 0.213333\n",
      "\tstep 3000: train loss 0.769846, train error 0.2\n",
      "\tstep 3200: train loss 0.654908, train error 0.153333\n",
      "\tstep 3400: train loss 0.684749, train error 0.196667\n",
      "\tstep 3600: train loss 0.648651, train error 0.18\n",
      "\tstep 3800: train loss 0.767636, train error 0.22\n",
      "\tstep 4000: train loss 0.546922, train error 0.13\n",
      "\tstep 4200: train loss 0.516512, train error 0.15\n",
      "\tstep 4400: train loss 0.521819, train error 0.136667\n",
      "\tstep 4600: train loss 0.540534, train error 0.156667\n",
      "\tstep 4800: train loss 0.528681, train error 0.156667\n",
      "\tstep 5000: train loss 0.538414, train error 0.15\n",
      "\tstep 5200: train loss 0.446984, train error 0.13\n",
      "\tstep 5400: train loss 0.431237, train error 0.12\n",
      "\tstep 5600: train loss 0.58578, train error 0.19\n",
      "\tstep 5800: train loss 0.455393, train error 0.126667\n",
      "\tstep 6000: train loss 0.437451, train error 0.12\n",
      "\tstep 6200: train loss 0.414191, train error 0.136667\n",
      "\tstep 6400: train loss 0.518155, train error 0.163333\n",
      "\tstep 6600: train loss 0.407027, train error 0.113333\n",
      "\tstep 6800: train loss 0.453326, train error 0.146667\n",
      "\tstep 7000: train loss 0.482669, train error 0.176667\n",
      "\tstep 7200: train loss 0.392431, train error 0.12\n",
      "\tstep 7400: train loss 0.351137, train error 0.1\n",
      "\tstep 7600: train loss 0.322471, train error 0.09\n",
      "\tstep 7800: train loss 0.385981, train error 0.13\n",
      "\tstep 8000: train loss 0.320038, train error 0.1\n",
      "\tstep 8200: train loss 0.310632, train error 0.1\n",
      "\tstep 8400: train loss 0.328999, train error 0.106667\n",
      "\tstep 8600: train loss 0.392099, train error 0.123333\n",
      "\tstep 8800: train loss 0.28142, train error 0.0866666\n",
      "\tstep 9000: train loss 0.258796, train error 0.07\n",
      "\tstep 9200: train loss 0.262266, train error 0.0766667\n",
      "\tstep 9400: train loss 0.340196, train error 0.143333\n",
      "\tstep 9600: train loss 0.320126, train error 0.103333\n",
      "\tstep 9800: train loss 0.250642, train error 0.0833333\n",
      "\tstep 10000: train loss 0.289936, train error 0.08\n",
      "\tstep 10200: train loss 0.254048, train error 0.07\n",
      "\tstep 10400: train loss 0.283846, train error 0.1\n",
      "\tstep 10600: train loss 0.262159, train error 0.0733333\n",
      "\tstep 10800: train loss 0.257001, train error 0.07\n",
      "\tstep 11000: train loss 0.23211, train error 0.0666667\n",
      "\tstep 11200: train loss 0.297404, train error 0.0866666\n",
      "\tstep 11400: train loss 0.266063, train error 0.0766667\n",
      "\tstep 11600: train loss 0.203413, train error 0.05\n",
      "\tstep 11800: train loss 0.227474, train error 0.0766667\n",
      "\tstep 12000: train loss 0.297351, train error 0.0966667\n",
      "\tstep 12200: train loss 0.198424, train error 0.0533333\n",
      "\tstep 12400: train loss 0.181218, train error 0.0566667\n",
      "\tstep 12600: train loss 0.164132, train error 0.0466667\n",
      "\tstep 12800: train loss 0.184905, train error 0.0466667\n",
      "\tstep 13000: train loss 0.242873, train error 0.0633333\n",
      "\tstep 13200: train loss 0.251168, train error 0.07\n",
      "\tstep 13400: train loss 0.263701, train error 0.0866666\n",
      "\tstep 13600: train loss 0.212668, train error 0.06\n",
      "\tstep 13800: train loss 0.206469, train error 0.07\n",
      "\tstep 14000: train loss 0.212482, train error 0.0666667\n",
      "\tstep 14200: train loss 0.258567, train error 0.0833333\n",
      "\tstep 14400: train loss 0.25312, train error 0.0766667\n",
      "\tstep 14600: train loss 0.1747, train error 0.0466667\n",
      "\tstep 14800: train loss 0.164467, train error 0.04\n",
      "\tstep 15000: train loss 0.179533, train error 0.0433334\n",
      "\tstep 15200: train loss 0.164004, train error 0.05\n",
      "\tstep 15400: train loss 0.190576, train error 0.0433334\n",
      "\tstep 15600: train loss 0.170021, train error 0.0533333\n",
      "\tstep 15800: train loss 0.176773, train error 0.05\n",
      "\tstep 16000: train loss 0.148227, train error 0.0266666\n",
      "\tstep 16200: train loss 0.184237, train error 0.06\n",
      "\tstep 16400: train loss 0.186502, train error 0.0533333\n",
      "\tstep 16600: train loss 0.243026, train error 0.0833333\n",
      "\tstep 16800: train loss 0.158196, train error 0.0366667\n",
      "\tstep 17000: train loss 0.179611, train error 0.0466667\n",
      "\tstep 17200: train loss 0.170817, train error 0.0433334\n",
      "\tstep 17400: train loss 0.152425, train error 0.0333334\n",
      "\tstep 17600: train loss 0.134324, train error 0.03\n",
      "\tstep 17800: train loss 0.154727, train error 0.0366667\n",
      "\tstep 18000: train loss 0.174745, train error 0.0466667\n",
      "\tstep 18200: train loss 0.189506, train error 0.05\n",
      "\tstep 18400: train loss 0.202111, train error 0.05\n",
      "\tstep 18600: train loss 0.12351, train error 0.0233333\n",
      "\tstep 18800: train loss 0.170018, train error 0.05\n",
      "\tstep 19000: train loss 0.118466, train error 0.0233333\n",
      "\tstep 19200: train loss 0.157212, train error 0.04\n",
      "\tstep 19400: train loss 0.124376, train error 0.03\n",
      "\tstep 19600: train loss 0.140409, train error 0.03\n",
      "\tstep 19800: train loss 0.142074, train error 0.0266666\n",
      "\tstep 20000: train loss 0.119595, train error 0.0266666\n",
      "\tstep 20200: train loss 0.175745, train error 0.0433334\n",
      "\tstep 20400: train loss 0.163225, train error 0.0466667\n",
      "\tstep 20600: train loss 0.172444, train error 0.0466667\n",
      "\tstep 20800: train loss 0.113975, train error 0.0233333\n",
      "\tstep 21000: train loss 0.115344, train error 0.0233333\n",
      "\tstep 21200: train loss 0.141269, train error 0.03\n",
      "\tstep 21400: train loss 0.125529, train error 0.0166667\n",
      "\tstep 21600: train loss 0.120747, train error 0.03\n",
      "\tstep 21800: train loss 0.235445, train error 0.06\n",
      "\tstep 22000: train loss 0.134426, train error 0.0266666\n",
      "\tstep 22200: train loss 0.160723, train error 0.0333334\n",
      "\tstep 22400: train loss 0.153362, train error 0.0433334\n",
      "\tstep 22600: train loss 0.0872389, train error 0.00666666\n",
      "\tstep 22800: train loss 0.0865889, train error 0.00333333\n",
      "\tstep 23000: train loss 0.146498, train error 0.0366667\n",
      "\tstep 23200: train loss 0.114177, train error 0.0133333\n",
      "\tstep 23400: train loss 0.108069, train error 0.0166667\n",
      "\tstep 23600: train loss 0.135405, train error 0.03\n",
      "\tstep 23800: train loss 0.131341, train error 0.0233333\n",
      "\tstep 24000: train loss 0.0936921, train error 0.00666666\n",
      "\tstep 24200: train loss 0.159849, train error 0.04\n",
      "\tstep 24400: train loss 0.114689, train error 0.0166667\n",
      "\tstep 24600: train loss 0.104946, train error 0.0166667\n",
      "\tstep 24800: train loss 0.206819, train error 0.06\n",
      "\tstep 25000: train loss 0.15374, train error 0.03\n",
      "\tstep 25200: train loss 0.135947, train error 0.0266666\n",
      "\tstep 25400: train loss 0.139392, train error 0.0333334\n",
      "\tstep 25600: train loss 0.131608, train error 0.0333334\n",
      "\tstep 25800: train loss 0.0931212, train error 0.0133333\n",
      "\tstep 26000: train loss 0.140515, train error 0.0233333\n",
      "\tstep 26200: train loss 0.142748, train error 0.0166667\n",
      "\tstep 26400: train loss 0.107674, train error 0.0166667\n",
      "\tstep 26600: train loss 0.116353, train error 0.0233333\n",
      "\tstep 26800: train loss 0.112228, train error 0.02\n",
      "\tstep 27000: train loss 0.105875, train error 0.0133333\n",
      "\tstep 27200: train loss 0.103829, train error 0.00999999\n",
      "\tstep 27400: train loss 0.127533, train error 0.0266666\n",
      "\tstep 27600: train loss 0.150048, train error 0.0266666\n",
      "\tstep 27800: train loss 0.119353, train error 0.02\n",
      "\tstep 28000: train loss 0.141695, train error 0.0366667\n",
      "\tstep 28200: train loss 0.120897, train error 0.0266666\n",
      "\tstep 28400: train loss 0.104722, train error 0.0133333\n",
      "\tstep 28600: train loss 0.127857, train error 0.0166667\n",
      "\tstep 28800: train loss 0.119801, train error 0.0233333\n",
      "\tstep 29000: train loss 0.110626, train error 0.0166667\n",
      "\tstep 29200: train loss 0.116501, train error 0.0233333\n",
      "\tstep 29400: train loss 0.1155, train error 0.0266666\n",
      "\tstep 29600: train loss 0.123171, train error 0.0166667\n",
      "\tstep 29800: train loss 0.10535, train error 0.0166667\n",
      "end training features // time elapsed: 11982.7850 s\n",
      "validation set error: 0.0526 // time elapsed: 1525465265.1847 s\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "f, f_params = get_feature_model(feature_model=GCNNLarge0, settings=Triplet_Hinge_Settings,\n",
    "                                num_features=10, data_format=[32, 32, 3],\n",
    "                                f_lr=0.04, f_reg=0.001, f_scope=\"f_gl0_cifar10_10\")\n",
    "fs = tf.train.Saver(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, \"f_gl0_cifar10_10\"))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    train_features(sess=sess, feature_model=f, samplers=s, num_steps=30000, keep_prob=0.7, f_params=f_params)\n",
    "    fs.save(sess, \"./models/cifar10/f_gl0_cifar10_10.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from features.settings import Baseline_Settings, Triplet_Hinge_Settings, Triplet_Log_Settings, Pair_Hinge_Settings\n",
    "from features.models import CNNSmall0, CNNSmall1, CNNSmall2, CNNMedium0, CNNMedium1, CNNLarge0, CNNLarge1\n",
    "from features.models import STNCNNSmall0, STNCNNSmall1, STNCNNSmall2, STNCNNMedium0, STNCNNMedium1\n",
    "from features.models import GCNNSmall0, GCNNMedium0, GCNNMedium1, GCNNLarge0\n",
    "from classes.models import LinearClassifier0, LinearClassifier1, LinearClassifier2, LinearClassifier3, TwoLayerClassifier0, TwoLayerClassifier0\n",
    "from learn import get_feature_model, get_classifier, train_features, train_classifier, test_model\n",
    "from datasets import load_fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Trying to share variable f_gm1_cifar10_10/wout, but specified shape (10, 100) and found shape (128, 10).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e064abf40f6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m f, f_params = get_feature_model(feature_model=GCNNMedium1, settings=Baseline_Settings,\n\u001b[1;32m      4\u001b[0m                                 \u001b[0mnum_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                                 f_lr=0.07, f_reg=0.001, f_scope=\"f_gm1_cifar10_10\")\n\u001b[0m\u001b[1;32m      6\u001b[0m c, c_params = get_classifier(classifier=LinearClassifier0, num_features=10, num_classes=10, \n\u001b[1;32m      7\u001b[0m                              c_lr=0.02, c_reg=0.001, c_scope=\"meh\", f_params=f_params)\n",
      "\u001b[0;32m~/thesis/similarity_experiments/learn.py\u001b[0m in \u001b[0;36mget_feature_model\u001b[0;34m(feature_model, settings, num_features, data_format, f_lr, f_reg, f_scope)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mf_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mf_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf_training\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_training\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/thesis/similarity_experiments/features/models.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, settings, data_format, num_features, lr, reg, dropout, training, scope)\u001b[0m\n\u001b[1;32m    635\u001b[0m                         \u001b[0;34m(\u001b[0m\u001b[0;34m\"wd2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m                         \u001b[0;34m(\u001b[0m\u001b[0;34m\"wout\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m                         (\"wout\", [num_features, 100])])\n\u001b[0m\u001b[1;32m    638\u001b[0m             self.biases = get_parameters(\n\u001b[1;32m    639\u001b[0m                     \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/thesis/similarity_experiments/features/models.py\u001b[0m in \u001b[0;36mget_parameters\u001b[0;34m(initializer, var_shape_pairs)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvar_shape_pairs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1201\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m       constraint=constraint)\n\u001b[0m\u001b[1;32m   1204\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m   1205\u001b[0m     \"\"\"%s\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1090\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m           \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1092\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m   1093\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m    423\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m           \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    392\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m           use_resource=use_resource, constraint=constraint)\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    745\u001b[0m         raise ValueError(\"Trying to share variable %s, but specified shape %s\"\n\u001b[1;32m    746\u001b[0m                          \" and found shape %s.\" % (name, shape,\n\u001b[0;32m--> 747\u001b[0;31m                                                    found_var.get_shape()))\n\u001b[0m\u001b[1;32m    748\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m         \u001b[0mdtype_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Trying to share variable f_gm1_cifar10_10/wout, but specified shape (10, 100) and found shape (128, 10)."
     ]
    }
   ],
   "source": [
    "# validation 0.0600\n",
    "tf.reset_default_graph()\n",
    "f, f_params = get_feature_model(feature_model=GCNNMedium1, settings=Baseline_Settings,\n",
    "                                num_features=10, data_format=[32, 32, 3],\n",
    "                                f_lr=0.07, f_reg=0.001, f_scope=\"f_gm1_cifar10_10\")\n",
    "c, c_params = get_classifier(classifier=LinearClassifier0, num_features=10, num_classes=10, \n",
    "                             c_lr=0.02, c_reg=0.001, c_scope=\"meh\", f_params=f_params)\n",
    "fs = tf.train.Saver(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, \"f_gm1_cifar10_10\"))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    fs.restore(sess, \"./models/cifar10/f_gm1_cifar10_10.ckpt\")\n",
    "    train_classifier(sess=sess, feature_model=f, classifier=c, data=d, samplers=s, num_steps=5000, \n",
    "                     f_params=f_params, c_params=c_params)\n",
    "    test_model(sess=sess, feature_model=f, classifier=c, samplers=s, data=d, f_params=f_params, c_params=c_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/cifar10/f_gm1_cifar10_10.ckpt\n",
      "begin training classifier\n",
      "\tstep 0: train loss 2.79635, train error 0.954286\n",
      "\tstep 100: train loss 1.89895, train error 0.73619\n",
      "\tstep 200: train loss 1.40349, train error 0.372857\n",
      "\tstep 300: train loss 1.13523, train error 0.214762\n",
      "\tstep 400: train loss 0.960114, train error 0.18381\n",
      "\tstep 500: train loss 0.865492, train error 0.177143\n",
      "\tstep 600: train loss 0.775687, train error 0.169524\n",
      "\tstep 700: train loss 0.761207, train error 0.185238\n",
      "\tstep 800: train loss 0.700037, train error 0.170952\n",
      "\tstep 900: train loss 0.663886, train error 0.167619\n",
      "\tstep 1000: train loss 0.629061, train error 0.167619\n",
      "\tstep 1100: train loss 0.631202, train error 0.166667\n",
      "\tstep 1200: train loss 0.609514, train error 0.165238\n",
      "\tstep 1300: train loss 0.596644, train error 0.165714\n",
      "\tstep 1400: train loss 0.576069, train error 0.161429\n",
      "\tstep 1500: train loss 0.570806, train error 0.15619\n",
      "\tstep 1600: train loss 0.559028, train error 0.161905\n",
      "\tstep 1700: train loss 0.589147, train error 0.174762\n",
      "\tstep 1800: train loss 0.550221, train error 0.159048\n",
      "\tstep 1900: train loss 0.580184, train error 0.17381\n",
      "\tstep 2000: train loss 0.534243, train error 0.158571\n",
      "\tstep 2100: train loss 0.601519, train error 0.187619\n",
      "\tstep 2200: train loss 0.557587, train error 0.169048\n",
      "\tstep 2300: train loss 0.584534, train error 0.17\n",
      "\tstep 2400: train loss 0.550207, train error 0.169048\n",
      "\tstep 2500: train loss 0.516615, train error 0.162381\n",
      "\tstep 2600: train loss 0.529598, train error 0.160952\n",
      "\tstep 2700: train loss 0.517758, train error 0.165238\n",
      "\tstep 2800: train loss 0.533387, train error 0.164762\n",
      "\tstep 2900: train loss 0.529386, train error 0.163333\n",
      "\tstep 3000: train loss 0.516508, train error 0.158571\n",
      "\tstep 3100: train loss 0.5264, train error 0.160476\n",
      "\tstep 3200: train loss 0.542854, train error 0.177619\n",
      "\tstep 3300: train loss 0.528076, train error 0.157143\n",
      "\tstep 3400: train loss 0.52492, train error 0.170476\n",
      "\tstep 3500: train loss 0.505291, train error 0.160476\n",
      "\tstep 3600: train loss 0.506448, train error 0.157619\n",
      "\tstep 3700: train loss 0.511403, train error 0.161905\n",
      "\tstep 3800: train loss 0.49994, train error 0.16\n",
      "\tstep 3900: train loss 0.526234, train error 0.16381\n",
      "\tstep 4000: train loss 0.550777, train error 0.186667\n",
      "\tstep 4100: train loss 0.485099, train error 0.149524\n",
      "\tstep 4200: train loss 0.515083, train error 0.161429\n",
      "\tstep 4300: train loss 0.527273, train error 0.172857\n",
      "\tstep 4400: train loss 0.503397, train error 0.159048\n",
      "\tstep 4500: train loss 0.49326, train error 0.155714\n",
      "\tstep 4600: train loss 0.508942, train error 0.15381\n",
      "\tstep 4700: train loss 0.525986, train error 0.173333\n",
      "\tstep 4800: train loss 0.525692, train error 0.172857\n",
      "\tstep 4900: train loss 0.506045, train error 0.16\n",
      "end training classifier // time elapsed: 1598.4088 s\n",
      "validation set error: 0.1068 // time elapsed: 0.6229 s\n",
      "train set error: 0.1415 // time elapsed: 10.3489 s\n",
      "validation set error: 0.1068 // time elapsed: 0.6075 s\n",
      "test set error: 0.1707 // time elapsed: 1.2139 s\n"
     ]
    }
   ],
   "source": [
    "# validation 0.0600\n",
    "tf.reset_default_graph()\n",
    "f, f_params = get_feature_model(feature_model=GCNNMedium1, settings=Baseline_Settings,\n",
    "                                num_features=10, data_format=[32, 32, 3],\n",
    "                                f_lr=0.07, f_reg=0.001, f_scope=\"f_gm1_cifar10_10\")\n",
    "c, c_params = get_classifier(classifier=LinearClassifier0, num_features=10, num_classes=10, \n",
    "                             c_lr=0.02, c_reg=0.001, c_scope=\"meh\", f_params=f_params)\n",
    "fs = tf.train.Saver(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, \"f_gm1_cifar10_10\"))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    fs.restore(sess, \"./models/cifar10/f_gm1_cifar10_10.ckpt\")\n",
    "    train_classifier(sess=sess, feature_model=f, classifier=c, data=d, samplers=s, num_steps=5000, \n",
    "                     f_params=f_params, c_params=c_params)\n",
    "    test_model(sess=sess, feature_model=f, classifier=c, samplers=s, data=d, f_params=f_params, c_params=c_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LinearClassifier0' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-05b2123f42d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                                 \u001b[0mnum_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                 f_lr=0.07, f_reg=0.001, f_scope=\"f_gl0_cifar10_10\")\n\u001b[0;32m----> 7\u001b[0;31m c, c_params = get_classifier(classifier=LinearClassifier0, num_features=10, num_classes=10, \n\u001b[0m\u001b[1;32m      8\u001b[0m                              c_lr=0.02, c_reg=0.001, c_scope=\"meh\", f_params=f_params)\n\u001b[1;32m      9\u001b[0m \u001b[0mfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGLOBAL_VARIABLES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"f_gl0_cifar10_10\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LinearClassifier0' is not defined"
     ]
    }
   ],
   "source": [
    "d=data; s=samplers\n",
    "# validation 0.0600\n",
    "tf.reset_default_graph()\n",
    "f, f_params = get_feature_model(feature_model=GCNNLarge0, settings=Baseline_Settings,\n",
    "                                num_features=10, data_format=[32, 32, 3],\n",
    "                                f_lr=0.07, f_reg=0.001, f_scope=\"f_gl0_cifar10_10\")\n",
    "c, c_params = get_classifier(classifier=LinearClassifier0, num_features=10, num_classes=10, \n",
    "                             c_lr=0.02, c_reg=0.001, c_scope=\"meh\", f_params=f_params)\n",
    "fs = tf.train.Saver(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, \"f_gl0_cifar10_10\"))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    fs.restore(sess, \"./models/cifar10/f_gl0_cifar10_10.ckpt\")\n",
    "    train_classifier(sess=sess, feature_model=f, classifier=c, data=d, samplers=s, num_steps=10000, \n",
    "                     f_params=f_params, c_params=c_params)\n",
    "    test_model(sess=sess, feature_model=f, classifier=c, samplers=s, data=d, f_params=f_params, c_params=c_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
