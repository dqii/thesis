{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dqi/.conda/envs/py36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/home/dqi/.conda/envs/py36/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.datasets import mnist, fashion_mnist\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_save_path(i):\n",
    "    return '/tmp/boostmodels' + str(i)\n",
    "\n",
    "def get_scope_variable(scope, var, shape=None, initializer=None):\n",
    "    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n",
    "        v = tf.get_variable(var, shape=shape, initializer=initializer)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_binary(y):\n",
    "    y_onehot = -np.ones((y.shape[0], 10))\n",
    "    y_onehot[np.arange(y.shape[0]), y] = 1\n",
    "    return y_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class OneVsAllMulticlassClassifier(object):\n",
    "    \n",
    "    def get_vars(i):\n",
    "        initializer = tf.contrib.layers.xavier_initializer(uniform=False)\n",
    "        W1 = get_scope_variable(scope=scope, var=\"W1_\" + str(i), shape=[784, num_features], initializer=initializer)\n",
    "        W2 = get_scope_variable(scope=scope, var=\"W2_\" + str(i), shape=[num_features, num_classes], initializer=initializer)\n",
    "        b1 = get_scope_variable(scope=scope, var=\"b1_\" + str(i), shape=[num_features], initializer=initializer)\n",
    "        b2 = get_scope_variable(scope=scope, var=\"b2_\" + str(i), shape=[num_classes], initializer=initializer)\n",
    "        return W1, W2, b1, b2\n",
    "    \n",
    "    def bn_fc_relu_fc(w1, w2, b1, b2, x):\n",
    "        return tf.matmul(tf.nn.relu(tf.matmul(tf.layers.batch_normalization(x), w1) + b1), w2) + b2\n",
    "    \n",
    "    def loss_fn(y, scores, reg, W1, W2):\n",
    "        return tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels=y, logits=scores)) + reg * (tf.nn.l2_loss(W1) + tf.nn.l2_loss(W2))\n",
    "    \n",
    "    def optimize(i):\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            self.optimizer.minimize(self.loss[i])\n",
    "    \n",
    "    def __init__(self, x, y, num_features, num_classes, lr, reg, scope=\"\"):\n",
    "        \"\"\" init the model with hyper-parameters etc \"\"\"\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.num_features = num_features\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.params = []\n",
    "        self.scores = []\n",
    "        self.losses = []\n",
    "        self.predictions = []\n",
    "        self.acc = []\n",
    "        self.incorrect = []\n",
    "        for i in range(10):\n",
    "            W1, W2, b1, b2 = get_vars(i)\n",
    "            self.params.append(get_vars(i))\n",
    "            self.scores.append(bn_fc_relu_fc(W1, W2, b1, b2, self.x))\n",
    "            self.losses.append(loss_fn(self.y[i], scores[i], reg, W1, W2))\n",
    "            \n",
    "            self.predictions.append(tf.argmax(scores[i], axis=1))\n",
    "            self.acc.append(tf.reduce_mean(tf.cast(tf.equal(self.predictions[i], self.y[i]), tf.float32)))\n",
    "            self.incorrect.append(tf.not_equal(self.predictions[i], self.y[i]))\n",
    "            self.metrics.append((self.losses[i], self.acc[i]))\n",
    "            \n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        self.optimizer = tf.train.AdamOptimizer(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_classifiers(data, iterations):\n",
    "    \n",
    "    (x_train, y_train), (x_test, y_test) = data\n",
    "    \n",
    "    probs = np.ones(60000, 10) / (60000 * 10)\n",
    "    alpha = np.zeros(iterations)    \n",
    "\n",
    "    for i in np.arange(iterations):\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        print(\"trial\", i, \"probability distribution:\\n\", \"%0.2f \"*10 % tuple(np.bincount(y_train, weights=probs)))\n",
    "        x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "        y = tf.placeholder(tf.int64, shape=[None, 10])\n",
    "        c = OneVsAllMulticlassClassifier(x=x, y=y, num_features=128, num_classes=10, lr=0.001, reg=0.005, scope=\"boosting\")\n",
    "        s = tf.train.Saver(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, \"boosting\"))\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            print(\"begin training classifier\")\n",
    "            train_time = time.time()\n",
    "            for step in range(2001):\n",
    "                indices = np.random.choice(60000 * 10, 1000, p=probs)\n",
    "                x_ = x_train[(indices/10).astype(int)]\n",
    "                y_ = y_train[(indices/10).astype(int)]\n",
    "                y_in = np.remainder(indices,10)\n",
    "                sess.run(c.optimize, feed_dict={x:x_, y:y_, y_in:y_in_})\n",
    "                if step % 1000 == 0:\n",
    "                    train_loss, train_acc = sess.run([c.loss, c.acc], feed_dict={x:x_, y:y_, y_in:y_in_})\n",
    "                    print(\"\\tepoch %d: train loss %g, train error %g\"%(step/60, train_loss, 1 - train_acc))  \n",
    "            train_time = time.time() - train_time\n",
    "            print(\"end training classifier // time elapsed: %.4f s\"%(train_time))\n",
    "\n",
    "            eval_train_time = time.time()\n",
    "            incorrect = sess.run(c.incorrect, feed_dict={x:x_train, y:y_train})\n",
    "            correct = incorrect * 2 - 1\n",
    "            train_error = np.sum(probs[incorrect.astype(bool)])\n",
    "            eval_train_time = time.time() - eval_train_time\n",
    "            print(\"train set error: %.4f // time elapsed: %.4f s\"%(train_error, eval_train_time))   \n",
    "\n",
    "            if train_error < 0.5:\n",
    "                s.save(sess=sess, save_path=get_save_path(i))\n",
    "            else:\n",
    "                return (i-1), alpha\n",
    "\n",
    "            eval_test_time = time.time()\n",
    "            test_error = 1 - sess.run(c.acc, feed_dict={x:x_test, y:y_test})\n",
    "            eval_test_time = time.time() - eval_test_time\n",
    "            print(\"test set error: %.4f // time elapsed: %.4f s\"%(test_error, eval_test_time)) \n",
    "\n",
    "            alpha[i] = 0.5 * np.log((1 - train_error)/train_error)\n",
    "            probs = probs * np.exp(alpha[i]*correct)\n",
    "            probs /= np.sum(probs)\n",
    "            \n",
    "    return iterations, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_classifiers(data, iterations, alpha): \n",
    "    \n",
    "    (x_train, y_train), (x_test, y_test) = data\n",
    "    scores = np.zeros((10000, 10))\n",
    "    \n",
    "    for i in np.arange(iterations):\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "        y = tf.placeholder(tf.int64, shape=[None])\n",
    "        c = MulticlassClassifier(x=x, y=y, num_features=128, num_classes=10, lr=0.001, reg=0.001, scope=\"boosting\")\n",
    "\n",
    "        s = tf.train.Saver(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, \"boosting\"))\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            s.restore(sess, get_save_path(i))\n",
    "            predictions = sess.run(c.predictions, feed_dict={x:x_test, y:y_test})\n",
    "            scores[np.arange(10000), predictions] += alpha[i]\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def boost(data, iterations):\n",
    "    iterations, alpha = train_classifiers(data, iterations)\n",
    "    scores = eval_classifiers(data, iterations, alpha)\n",
    "    print(\"iterations:\", iterations, \"| boosted accuracy:\", np.mean(np.equal(np.argmax(scores, axis=1), y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dqi/.conda/envs/py36/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/dqi/.conda/envs/py36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras as keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten\n",
    "from keras import regularizers\n",
    "from keras.datasets import mnist, fashion_mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape((-1, 28, 28, 1))\n",
    "x_test = x_test.reshape((-1, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_binary_classifier():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=3, strides=1, activation='relu', input_shape=(28, 28, 1)))\n",
    "    model.add(Conv2D(64, kernel_size=3, strides=1, activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    model.add(Conv2D(64, kernel_size=3, strides=1, activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.02)))\n",
    "    model.add(Dense(10, activation='relu', kernel_regularizer=regularizers.l2(0.02)))\n",
    "    model.add(Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(0.01)))\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def get_learners(num_classes):\n",
    "    models = []\n",
    "    for _ in range(num_classes):\n",
    "        models.append(get_binary_classifier())\n",
    "    return models\n",
    "              \n",
    "def del_models(models):\n",
    "    for i in range(len(models)):\n",
    "        del models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH: 0 PROBS: 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 \n",
      "class 0: train err 0.00, test err 1.00\n",
      "class 1: train err 0.02, test err 1.00\n",
      "class 2: train err 0.02, test err 1.00\n",
      "class 3: train err 0.02, test err 1.00\n",
      "class 4: train err 0.02, test err 1.00\n",
      "class 5: train err 0.02, test err 1.00\n",
      "class 6: train err 0.02, test err 1.00\n",
      "class 7: train err 0.02, test err 1.00\n",
      "class 8: train err 0.02, test err 1.00\n",
      "class 9: train err 0.02, test err 0.99\n",
      "weighted error: 0.17829166666666663\n",
      "test error: 0.0116\n",
      "\n",
      "EPOCH: 1 PROBS: 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 \n",
      "class 0: train err 0.00, test err 1.00\n",
      "class 1: train err 0.02, test err 1.00\n",
      "class 2: train err 0.02, test err 1.00\n",
      "class 3: train err 0.02, test err 1.00\n",
      "class 4: train err 0.02, test err 1.00\n",
      "class 5: train err 0.02, test err 1.00\n",
      "class 6: train err 0.02, test err 1.00\n",
      "class 7: train err 0.02, test err 1.00\n",
      "class 8: train err 0.02, test err 1.00\n",
      "class 9: train err 0.02, test err 1.00\n",
      "weighted error: 0.1783927856217372\n",
      "test error: 0.0091\n",
      "\n",
      "EPOCH: 2 PROBS: 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 \n",
      "class 0: train err 0.00, test err 1.00\n",
      "class 1: train err 0.02, test err 1.00\n",
      "class 2: train err 0.02, test err 1.00\n",
      "class 3: train err 0.02, test err 1.00\n",
      "class 4: train err 0.02, test err 1.00\n",
      "class 5: train err 0.02, test err 1.00\n",
      "class 6: train err 0.02, test err 1.00\n",
      "class 7: train err 0.02, test err 1.00\n",
      "class 8: train err 0.02, test err 1.00\n",
      "class 9: train err 0.02, test err 0.99\n",
      "weighted error: 0.1787696995351606\n",
      "test error: 0.0077\n",
      "\n",
      "EPOCH: 3 PROBS: 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 \n",
      "class 0: train err 0.00, test err 1.00\n",
      "class 1: train err 0.02, test err 1.00\n",
      "class 2: train err 0.02, test err 1.00\n",
      "class 3: train err 0.02, test err 1.00\n",
      "class 4: train err 0.02, test err 1.00\n",
      "class 5: train err 0.02, test err 1.00\n",
      "class 6: train err 0.02, test err 1.00\n",
      "class 7: train err 0.02, test err 1.00\n",
      "class 8: train err 0.02, test err 1.00\n",
      "class 9: train err 0.02, test err 0.99\n",
      "weighted error: 0.17908219844628756\n",
      "test error: 0.0067\n",
      "\n",
      "EPOCH: 4 PROBS: 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 \n",
      "class 0: train err 0.00, test err 1.00\n",
      "class 1: train err 0.02, test err 1.00\n",
      "class 2: train err 0.02, test err 1.00\n",
      "class 3: train err 0.02, test err 1.00\n",
      "class 4: train err 0.02, test err 1.00\n",
      "class 5: train err 0.02, test err 1.00\n",
      "class 6: train err 0.02, test err 1.00\n",
      "class 7: train err 0.02, test err 1.00\n",
      "class 8: train err 0.02, test err 1.00\n",
      "class 9: train err 0.02, test err 1.00\n",
      "weighted error: 0.1780303246438678\n",
      "test error: 0.0064\n",
      "\n",
      "EPOCH: 5 PROBS: 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 \n",
      "class 0: train err 0.00, test err 1.00\n",
      "class 1: train err 0.02, test err 1.00\n",
      "class 2: train err 0.02, test err 1.00\n",
      "class 3: train err 0.02, test err 1.00\n",
      "class 4: train err 0.02, test err 1.00\n",
      "class 5: train err 0.02, test err 1.00\n",
      "class 6: train err 0.02, test err 1.00\n",
      "class 7: train err 0.02, test err 1.00\n",
      "class 8: train err 0.02, test err 1.00\n",
      "class 9: train err 0.02, test err 1.00\n",
      "weighted error: 0.17822324338365902\n",
      "test error: 0.0065\n",
      "\n",
      "EPOCH: 6 PROBS: 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 \n",
      "class 0: train err 0.00, test err 1.00\n",
      "class 1: train err 0.02, test err 1.00\n",
      "class 2: train err 0.02, test err 1.00\n",
      "class 3: train err 0.02, test err 1.00\n",
      "class 4: train err 0.02, test err 1.00\n",
      "class 5: train err 0.02, test err 1.00\n",
      "class 6: train err 0.02, test err 1.00\n",
      "class 7: train err 0.02, test err 1.00\n",
      "class 8: train err 0.02, test err 1.00\n",
      "class 9: train err 0.02, test err 1.00\n",
      "weighted error: 0.17836512562002482\n",
      "test error: 0.0064\n",
      "\n",
      "EPOCH: 7 PROBS: 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 \n",
      "class 0: train err 0.00, test err 1.00\n",
      "class 1: train err 0.02, test err 1.00\n",
      "class 2: train err 0.02, test err 1.00\n",
      "class 3: train err 0.02, test err 1.00\n",
      "class 4: train err 0.02, test err 1.00\n",
      "class 5: train err 0.02, test err 1.00\n",
      "class 6: train err 0.02, test err 1.00\n",
      "class 7: train err 0.02, test err 1.00\n",
      "class 8: train err 0.02, test err 1.00\n",
      "class 9: train err 0.02, test err 1.00\n",
      "weighted error: 0.17761652276454312\n",
      "test error: 0.0062\n",
      "\n",
      "EPOCH: 8 PROBS: 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 \n",
      "class 0: train err 0.00, test err 1.00\n",
      "class 1: train err 0.02, test err 1.00\n",
      "class 2: train err 0.02, test err 1.00\n",
      "class 3: train err 0.02, test err 1.00\n",
      "class 4: train err 0.02, test err 1.00\n",
      "class 5: train err 0.02, test err 1.00\n",
      "class 6: train err 0.02, test err 1.00\n",
      "class 7: train err 0.02, test err 1.00\n",
      "class 8: train err 0.02, test err 1.00\n",
      "class 9: train err 0.02, test err 0.99\n",
      "weighted error: 0.17795554570400066\n",
      "test error: 0.0062\n",
      "\n",
      "EPOCH: 9 PROBS: 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 \n",
      "class 0: train err 0.00, test err 1.00\n",
      "class 1: train err 0.02, test err 1.00\n",
      "class 2: train err 0.02, test err 1.00\n",
      "class 3: train err 0.02, test err 1.00\n",
      "class 4: train err 0.02, test err 1.00\n",
      "class 5: train err 0.02, test err 1.00\n",
      "class 6: train err 0.02, test err 1.00\n",
      "class 7: train err 0.02, test err 1.00\n",
      "class 8: train err 0.02, test err 1.00\n",
      "class 9: train err 0.02, test err 1.00\n",
      "weighted error: 0.17846124448323444\n",
      "test error: 0.0062\n",
      "\n",
      "EPOCH: 10 PROBS: 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 \n",
      "class 0: train err 0.00, test err 1.00\n",
      "class 1: train err 0.02, test err 1.00\n",
      "class 2: train err 0.02, test err 1.00\n",
      "class 3: train err 0.02, test err 1.00\n",
      "class 4: train err 0.02, test err 1.00\n",
      "class 5: train err 0.02, test err 1.00\n",
      "class 6: train err 0.02, test err 1.00\n",
      "class 7: train err 0.02, test err 1.00\n",
      "class 8: train err 0.02, test err 1.00\n",
      "class 9: train err 0.02, test err 1.00\n",
      "weighted error: 0.17772387964046607\n",
      "test error: 0.0057\n",
      "\n",
      "EPOCH: 11 PROBS: 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 \n",
      "class 0: train err 0.00, test err 1.00\n",
      "class 1: train err 0.02, test err 1.00\n",
      "class 2: train err 0.02, test err 1.00\n",
      "class 3: train err 0.02, test err 1.00\n",
      "class 4: train err 0.02, test err 1.00\n",
      "class 5: train err 0.02, test err 1.00\n",
      "class 6: train err 0.02, test err 1.00\n",
      "class 7: train err 0.02, test err 1.00\n",
      "class 8: train err 0.02, test err 1.00\n",
      "class 9: train err 0.02, test err 1.00\n",
      "weighted error: 0.17803835169276996\n",
      "test error: 0.0058\n",
      "\n",
      "EPOCH: 12 PROBS: 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 \n",
      "class 0: train err 0.00, test err 1.00\n",
      "class 1: train err 0.02, test err 1.00\n",
      "class 2: train err 0.02, test err 1.00\n",
      "class 3: train err 0.02, test err 0.99\n",
      "class 4: train err 0.02, test err 1.00\n",
      "class 5: train err 0.02, test err 1.00\n",
      "class 6: train err 0.02, test err 1.00\n",
      "class 7: train err 0.02, test err 1.00\n",
      "class 8: train err 0.02, test err 1.00\n",
      "class 9: train err 0.02, test err 1.00\n",
      "weighted error: 0.17693813155507038\n",
      "test error: 0.0058\n",
      "\n",
      "EPOCH: 13 PROBS: 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 \n",
      "class 0: train err 0.00, test err 1.00\n",
      "class 1: train err 0.02, test err 1.00\n",
      "class 2: train err 0.02, test err 1.00\n",
      "class 3: train err 0.02, test err 1.00\n",
      "class 4: train err 0.02, test err 1.00\n",
      "class 5: train err 0.02, test err 1.00\n",
      "class 6: train err 0.02, test err 1.00\n",
      "class 7: train err 0.02, test err 1.00\n",
      "class 8: train err 0.02, test err 1.00\n",
      "class 9: train err 0.02, test err 0.99\n",
      "weighted error: 0.17754977960226467\n",
      "test error: 0.0059\n",
      "\n",
      "EPOCH: 14 PROBS: 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 \n",
      "class 0: train err 0.00, test err 0.99\n",
      "class 1: train err 0.02, test err 1.00\n",
      "class 2: train err 0.02, test err 1.00\n",
      "class 3: train err 0.02, test err 1.00\n",
      "class 4: train err 0.02, test err 1.00\n",
      "class 5: train err 0.02, test err 1.00\n",
      "class 6: train err 0.02, test err 1.00\n",
      "class 7: train err 0.02, test err 1.00\n",
      "class 8: train err 0.02, test err 1.00\n",
      "class 9: train err 0.02, test err 1.00\n",
      "weighted error: 0.17793948515065255\n",
      "test error: 0.0056\n",
      "\n",
      "EPOCH: 15 PROBS: 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 \n",
      "class 0: train err 0.00, test err 1.00\n",
      "class 1: train err 0.02, test err 1.00\n",
      "class 2: train err 0.02, test err 1.00\n",
      "class 3: train err 0.02, test err 1.00\n",
      "class 4: train err 0.02, test err 1.00\n",
      "class 5: train err 0.02, test err 1.00\n",
      "class 6: train err 0.02, test err 1.00\n",
      "class 7: train err 0.02, test err 1.00\n",
      "class 8: train err 0.02, test err 1.00\n",
      "class 9: train err 0.02, test err 0.99\n",
      "weighted error: 0.177816120408269\n",
      "test error: 0.0057\n",
      "\n",
      "EPOCH: 16 PROBS: 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class 0: train err 0.00, test err 1.00\n",
      "class 1: train err 0.02, test err 1.00\n",
      "class 2: train err 0.02, test err 1.00\n",
      "class 3: train err 0.02, test err 1.00\n",
      "class 4: train err 0.02, test err 1.00\n",
      "class 5: train err 0.02, test err 1.00\n",
      "class 6: train err 0.02, test err 1.00\n",
      "class 7: train err 0.02, test err 1.00\n",
      "class 8: train err 0.02, test err 1.00\n",
      "class 9: train err 0.02, test err 1.00\n",
      "weighted error: 0.17738361375126369\n",
      "test error: 0.0056\n",
      "\n",
      "EPOCH: 17 PROBS: 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 \n",
      "class 0: train err 0.00, test err 1.00\n",
      "class 1: train err 0.02, test err 1.00\n",
      "class 2: train err 0.02, test err 1.00\n",
      "class 3: train err 0.02, test err 1.00\n",
      "class 4: train err 0.02, test err 1.00\n",
      "class 5: train err 0.02, test err 1.00\n",
      "class 6: train err 0.02, test err 1.00\n"
     ]
    }
   ],
   "source": [
    "models = get_learners(10)\n",
    "probs = np.ones(60000*10) / (60000*10)\n",
    "num_iterations = 20\n",
    "alpha = np.zeros(num_iterations)\n",
    "H = np.zeros((10000, 10)) # for testing\n",
    "\n",
    "for round in range(20):\n",
    "    print(\"\\nEPOCH:\", round, \"PROBS:\", \"%0.2f \"*10 % tuple(np.sum(probs.reshape((60000,10)), axis=0)))\n",
    "\n",
    "    # sample from distribution\n",
    "    indices = np.random.choice(60000*10, 600000, p=probs)\n",
    "    \n",
    "    # parameters needed to update distribution\n",
    "    h = np.zeros((60000,10))\n",
    "    \n",
    "    # train models\n",
    "    error = 0\n",
    "    for i in range(10):\n",
    "        idx = np.ndarray.flatten(np.array(np.where(indices % 10 == i)))\n",
    "        examples = (idx/10).astype(int)\n",
    "        x = x_train[examples]\n",
    "        y = (y_train == i)[examples]\n",
    "        models[i].fit(x, y, epochs=5, batch_size=128, verbose=0)\n",
    "        test_loss, test_err = models[i].evaluate(x_test, (y_test == i), batch_size=128, verbose=0)        \n",
    "        \n",
    "        # calculate error\n",
    "        output = models[i].predict(x_train, batch_size=128).reshape(60000)\n",
    "        h[:,i] = output-0.5\n",
    "        incorrect = ((h[:,i] > 0) != (y_train == 0))\n",
    "        train_err = np.sum(probs.reshape((60000,10))[incorrect.reshape(60000), 0])         \n",
    "        error += train_err\n",
    "        \n",
    "        print(\"class %d: train err %0.2f, test err %0.2f\"%(i, train_err, test_err))\n",
    "    \n",
    "    # update distribution   \n",
    "    print(\"weighted error:\", error)\n",
    "    alpha[round] = 0.5 * np.log((1-error)/error)\n",
    "    \n",
    "    Y = -np.ones((60000,10))\n",
    "    Y[np.arange(60000), y_train] = 1\n",
    "    Y = np.ndarray.flatten(Y)\n",
    "        \n",
    "    probs = probs * np.exp(alpha[round] * Y * h.reshape(600000))\n",
    "    probs /= np.sum(probs)\n",
    "    \n",
    "    # test    \n",
    "    for i in range(10):\n",
    "        output = models[i].predict(x_test, batch_size=128).reshape(10000)\n",
    "        H[:,i] += alpha[round] * output-0.5\n",
    "    print(\"test error:\", np.mean(np.argmax(H, axis=1) != y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def boost(data, num_iterations):\n",
    "    (x_train, y_train), (x_test, y_test) = data\n",
    "    num_train = y_train.shape[0]\n",
    "    num_test = y_test.shape[0]\n",
    "    num_classes = len(np.unique(y_train))\n",
    "\n",
    "    models = get_learners(num_classes)\n",
    "    probs = np.ones(num_train*num_classes) / (num_train*num_classes)\n",
    "\n",
    "    # for testing\n",
    "    alpha = np.zeros(num_iterations) \n",
    "    H = np.zeros((num_test, num_classes)) \n",
    "\n",
    "    for round in range(num_iterations):\n",
    "        print(\"\\nEPOCH:\", round, \"PROBS:\", \"%0.2f \"*10 % tuple(np.sum(probs.reshape((num_train, num_classes)), axis=0)))\n",
    "\n",
    "        # sample from distribution\n",
    "        indices = np.random.choice(num_train*num_classes, num_train*num_classes, p=probs)\n",
    "\n",
    "        # parameters needed to update distribution\n",
    "        h = np.zeros((num_train, num_classes))\n",
    "\n",
    "        # train models\n",
    "        error = 0\n",
    "        for i in range(num_classes):        \n",
    "            idx = np.ndarray.flatten(np.array(np.where(indices % num_classes == i)))\n",
    "            examples = (idx/num_classes).astype(int)\n",
    "            x = x_train[examples]\n",
    "            y = (y_train == i)[examples]        \n",
    "            models[i].fit(x, y, epochs=5, batch_size=128, verbose=0)\n",
    "            test_loss, test_acc = models[i].evaluate(x_test, (y_test == i), batch_size=128, verbose=0)\n",
    "            test_err = 1 - test_acc\n",
    "        \n",
    "            # calculate error\n",
    "            output = models[i].predict(x_train, batch_size=128).reshape(num_train)\n",
    "            h[:,i] = output-0.5\n",
    "            incorrect = ((h[:,i] > 0) != (y_train == 0))\n",
    "            train_err = np.sum(probs.reshape((num_train,num_classes))[incorrect.reshape(num_train), 0])         \n",
    "            error += train_err\n",
    "\n",
    "            print(\"class %d: train err %0.2f, test err %0.2f\"%(i, train_err, test_err))\n",
    " \n",
    "        # update distribution   \n",
    "        print(\"weighted error:\", error)\n",
    "        alpha[round] = 0.5 * np.log((1-error)/error)\n",
    "    \n",
    "        Y = -np.ones((num_train, num_classes))\n",
    "        Y[np.arange(num_train), y_train] = 1\n",
    "        Y = np.ndarray.flatten(Y)\n",
    "        \n",
    "        probs = probs * np.exp(alpha[round] * Y * h.reshape(num_train*num_classes))\n",
    "        probs /= np.sum(probs)\n",
    "    \n",
    "        # test    \n",
    "        for i in range(num_classes):\n",
    "            output = models[i].predict(x_test, batch_size=128).reshape(num_test)\n",
    "            H[:,i] += alpha[round] * output-0.5\n",
    "        print(\"test error:\", np.mean(np.argmax(H, axis=1) != y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape((-1, 28, 28, 1))\n",
    "x_test = x_test.reshape((-1, 28, 28, 1))\n",
    "data = (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH: 0 PROBS: 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 \n",
      "class 0: train err 0.00, test err 0.00\n",
      "class 1: train err 0.02, test err 0.00\n",
      "class 2: train err 0.02, test err 0.01\n",
      "class 3: train err 0.02, test err 0.00\n",
      "class 4: train err 0.02, test err 0.00\n",
      "class 5: train err 0.02, test err 0.00\n",
      "class 6: train err 0.02, test err 0.00\n",
      "class 7: train err 0.02, test err 0.00\n",
      "class 8: train err 0.02, test err 0.00\n",
      "class 9: train err 0.02, test err 0.01\n",
      "weighted error: 0.17827833333333326\n",
      "test error: 0.0132\n",
      "\n",
      "EPOCH: 1 PROBS: 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 \n",
      "class 0: train err 0.00, test err 0.00\n",
      "class 1: train err 0.02, test err 0.00\n",
      "class 2: train err 0.02, test err 0.00\n",
      "class 3: train err 0.02, test err 0.00\n",
      "class 4: train err 0.02, test err 0.00\n",
      "class 5: train err 0.02, test err 0.00\n",
      "class 6: train err 0.02, test err 0.00\n",
      "class 7: train err 0.02, test err 0.00\n",
      "class 8: train err 0.02, test err 0.00\n",
      "class 9: train err 0.02, test err 0.00\n",
      "weighted error: 0.17909052096443606\n",
      "test error: 0.0106\n",
      "\n",
      "EPOCH: 2 PROBS: 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 \n",
      "class 0: train err 0.00, test err 0.00\n",
      "class 1: train err 0.02, test err 0.00\n",
      "class 2: train err 0.02, test err 0.00\n",
      "class 3: train err 0.02, test err 0.00\n",
      "class 4: train err 0.02, test err 0.00\n",
      "class 5: train err 0.02, test err 0.00\n",
      "class 6: train err 0.02, test err 0.00\n",
      "class 7: train err 0.02, test err 0.00\n",
      "class 8: train err 0.02, test err 0.01\n",
      "class 9: train err 0.02, test err 0.01\n",
      "weighted error: 0.1794893838638347\n",
      "test error: 0.0095\n",
      "\n",
      "EPOCH: 3 PROBS: 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 \n",
      "class 0: train err 0.00, test err 0.00\n",
      "class 1: train err 0.02, test err 0.00\n",
      "class 2: train err 0.02, test err 0.00\n",
      "class 3: train err 0.02, test err 0.00\n",
      "class 4: train err 0.02, test err 0.00\n",
      "class 5: train err 0.02, test err 0.00\n",
      "class 6: train err 0.02, test err 0.00\n",
      "class 7: train err 0.02, test err 0.00\n",
      "class 8: train err 0.02, test err 0.00\n",
      "class 9: train err 0.02, test err 0.00\n",
      "weighted error: 0.1790977630755894\n",
      "test error: 0.0087\n",
      "\n",
      "EPOCH: 4 PROBS: 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 \n",
      "class 0: train err 0.00, test err 0.00\n",
      "class 1: train err 0.02, test err 0.00\n",
      "class 2: train err 0.02, test err 0.00\n",
      "class 3: train err 0.02, test err 0.00\n",
      "class 4: train err 0.02, test err 0.00\n",
      "class 5: train err 0.02, test err 0.00\n",
      "class 6: train err 0.02, test err 0.00\n",
      "class 7: train err 0.02, test err 0.00\n",
      "class 8: train err 0.02, test err 0.00\n",
      "class 9: train err 0.02, test err 0.00\n",
      "weighted error: 0.17882829293696534\n",
      "test error: 0.0082\n",
      "\n",
      "EPOCH: 5 PROBS: 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 \n",
      "class 0: train err 0.00, test err 0.00\n",
      "class 1: train err 0.02, test err 0.00\n",
      "class 2: train err 0.02, test err 0.00\n",
      "class 3: train err 0.02, test err 0.00\n",
      "class 4: train err 0.02, test err 0.00\n",
      "class 5: train err 0.02, test err 0.00\n",
      "class 6: train err 0.02, test err 0.00\n",
      "class 7: train err 0.02, test err 0.00\n",
      "class 8: train err 0.02, test err 0.00\n",
      "class 9: train err 0.02, test err 0.01\n",
      "weighted error: 0.1793935393370438\n",
      "test error: 0.0072\n",
      "\n",
      "EPOCH: 6 PROBS: 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 \n",
      "class 0: train err 0.00, test err 0.00\n",
      "class 1: train err 0.02, test err 0.00\n",
      "class 2: train err 0.02, test err 0.00\n",
      "class 3: train err 0.02, test err 0.00\n",
      "class 4: train err 0.02, test err 0.00\n",
      "class 5: train err 0.02, test err 0.00\n",
      "class 6: train err 0.02, test err 0.00\n",
      "class 7: train err 0.02, test err 0.00\n",
      "class 8: train err 0.02, test err 0.00\n",
      "class 9: train err 0.02, test err 0.00\n",
      "weighted error: 0.17886351871685774\n",
      "test error: 0.0072\n",
      "\n",
      "EPOCH: 7 PROBS: 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 \n",
      "class 0: train err 0.00, test err 0.00\n",
      "class 1: train err 0.02, test err 0.00\n",
      "class 2: train err 0.02, test err 0.00\n",
      "class 3: train err 0.02, test err 0.00\n",
      "class 4: train err 0.02, test err 0.00\n",
      "class 5: train err 0.02, test err 0.00\n",
      "class 6: train err 0.02, test err 0.00\n",
      "class 7: train err 0.02, test err 0.00\n",
      "class 8: train err 0.02, test err 0.00\n",
      "class 9: train err 0.02, test err 0.00\n",
      "weighted error: 0.17890734948548537\n",
      "test error: 0.0069\n",
      "\n",
      "EPOCH: 8 PROBS: 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 \n",
      "class 0: train err 0.00, test err 0.00\n",
      "class 1: train err 0.02, test err 0.00\n",
      "class 2: train err 0.02, test err 0.00\n",
      "class 3: train err 0.02, test err 0.00\n",
      "class 4: train err 0.02, test err 0.00\n",
      "class 5: train err 0.02, test err 0.00\n",
      "class 6: train err 0.02, test err 0.00\n",
      "class 7: train err 0.02, test err 0.00\n",
      "class 8: train err 0.02, test err 0.00\n",
      "class 9: train err 0.02, test err 0.00\n",
      "weighted error: 0.1786547924905623\n",
      "test error: 0.0064\n",
      "\n",
      "EPOCH: 9 PROBS: 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 \n",
      "class 0: train err 0.00, test err 0.00\n",
      "class 1: train err 0.02, test err 0.00\n",
      "class 2: train err 0.02, test err 0.00\n",
      "class 3: train err 0.02, test err 0.00\n",
      "class 4: train err 0.02, test err 0.00\n",
      "class 5: train err 0.02, test err 0.00\n",
      "class 6: train err 0.02, test err 0.00\n",
      "class 7: train err 0.02, test err 0.00\n",
      "class 8: train err 0.02, test err 0.00\n",
      "class 9: train err 0.02, test err 0.00\n",
      "weighted error: 0.17839830933403744\n",
      "test error: 0.0061\n"
     ]
    }
   ],
   "source": [
    "boost(data, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fashion MNIST Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "x_train = x_train.reshape((-1, 28, 28, 1))\n",
    "x_test = x_test.reshape((-1, 28, 28, 1))\n",
    "data = (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH: 0 PROBS: 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 \n",
      "class 0: train err 0.00, test err 0.03\n",
      "class 1: train err 0.02, test err 0.00\n",
      "class 2: train err 0.02, test err 0.03\n",
      "class 3: train err 0.02, test err 0.02\n",
      "class 4: train err 0.02, test err 0.04\n",
      "class 5: train err 0.02, test err 0.01\n",
      "class 6: train err 0.02, test err 0.06\n",
      "class 7: train err 0.02, test err 0.01\n",
      "class 8: train err 0.02, test err 0.01\n",
      "class 9: train err 0.02, test err 0.01\n",
      "weighted error: 0.17796499999999998\n",
      "test error: 0.1005\n",
      "\n",
      "EPOCH: 1 PROBS: 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 \n",
      "class 0: train err 0.00, test err 0.03\n",
      "class 1: train err 0.02, test err 0.00\n",
      "class 2: train err 0.02, test err 0.03\n",
      "class 3: train err 0.02, test err 0.02\n",
      "class 4: train err 0.02, test err 0.03\n",
      "class 5: train err 0.02, test err 0.01\n",
      "class 6: train err 0.02, test err 0.05\n",
      "class 7: train err 0.02, test err 0.01\n",
      "class 8: train err 0.02, test err 0.00\n",
      "class 9: train err 0.02, test err 0.01\n",
      "weighted error: 0.1674420167794555\n",
      "test error: 0.0864\n",
      "\n",
      "EPOCH: 2 PROBS: 0.10 0.10 0.10 0.10 0.10 0.10 0.09 0.10 0.10 0.10 \n",
      "class 0: train err 0.00, test err 0.03\n",
      "class 1: train err 0.02, test err 0.00\n",
      "class 2: train err 0.02, test err 0.03\n",
      "class 3: train err 0.02, test err 0.02\n",
      "class 4: train err 0.02, test err 0.03\n",
      "class 5: train err 0.02, test err 0.00\n",
      "class 6: train err 0.02, test err 0.05\n",
      "class 7: train err 0.02, test err 0.01\n",
      "class 8: train err 0.02, test err 0.00\n",
      "class 9: train err 0.02, test err 0.01\n",
      "weighted error: 0.16187455332770617\n",
      "test error: 0.0806\n",
      "\n",
      "EPOCH: 3 PROBS: 0.10 0.10 0.10 0.10 0.10 0.10 0.09 0.10 0.10 0.10 \n",
      "class 0: train err 0.00, test err 0.03\n",
      "class 1: train err 0.02, test err 0.00\n",
      "class 2: train err 0.02, test err 0.03\n",
      "class 3: train err 0.02, test err 0.02\n",
      "class 4: train err 0.02, test err 0.03\n",
      "class 5: train err 0.02, test err 0.00\n",
      "class 6: train err 0.02, test err 0.05\n",
      "class 7: train err 0.02, test err 0.01\n",
      "class 8: train err 0.02, test err 0.00\n",
      "class 9: train err 0.02, test err 0.01\n",
      "weighted error: 0.15962205441971958\n",
      "test error: 0.076\n",
      "\n",
      "EPOCH: 4 PROBS: 0.10 0.10 0.10 0.10 0.10 0.10 0.09 0.10 0.10 0.10 \n",
      "class 0: train err 0.00, test err 0.03\n",
      "class 1: train err 0.02, test err 0.00\n",
      "class 2: train err 0.02, test err 0.03\n",
      "class 3: train err 0.02, test err 0.02\n",
      "class 4: train err 0.02, test err 0.03\n",
      "class 5: train err 0.02, test err 0.00\n",
      "class 6: train err 0.02, test err 0.06\n",
      "class 7: train err 0.02, test err 0.01\n",
      "class 8: train err 0.02, test err 0.00\n",
      "class 9: train err 0.02, test err 0.01\n",
      "weighted error: 0.15846138930463932\n",
      "test error: 0.0746\n",
      "\n",
      "EPOCH: 5 PROBS: 0.10 0.10 0.10 0.10 0.10 0.10 0.09 0.10 0.10 0.10 \n",
      "class 0: train err 0.00, test err 0.03\n",
      "class 1: train err 0.02, test err 0.00\n",
      "class 2: train err 0.02, test err 0.03\n",
      "class 3: train err 0.02, test err 0.02\n",
      "class 4: train err 0.02, test err 0.03\n",
      "class 5: train err 0.02, test err 0.01\n",
      "class 6: train err 0.01, test err 0.05\n",
      "class 7: train err 0.02, test err 0.01\n",
      "class 8: train err 0.02, test err 0.00\n",
      "class 9: train err 0.02, test err 0.01\n",
      "weighted error: 0.15644040140096113\n",
      "test error: 0.0712\n",
      "\n",
      "EPOCH: 6 PROBS: 0.10 0.10 0.10 0.10 0.10 0.10 0.09 0.10 0.10 0.10 \n",
      "class 0: train err 0.00, test err 0.03\n",
      "class 1: train err 0.02, test err 0.00\n",
      "class 2: train err 0.02, test err 0.03\n",
      "class 3: train err 0.02, test err 0.02\n",
      "class 4: train err 0.02, test err 0.03\n",
      "class 5: train err 0.02, test err 0.01\n",
      "class 6: train err 0.02, test err 0.05\n",
      "class 7: train err 0.02, test err 0.01\n",
      "class 8: train err 0.02, test err 0.00\n",
      "class 9: train err 0.02, test err 0.01\n",
      "weighted error: 0.15521347402847468\n",
      "test error: 0.0704\n",
      "\n",
      "EPOCH: 7 PROBS: 0.10 0.10 0.10 0.10 0.10 0.10 0.09 0.10 0.10 0.10 \n",
      "class 0: train err 0.00, test err 0.03\n",
      "class 1: train err 0.02, test err 0.00\n",
      "class 2: train err 0.02, test err 0.03\n",
      "class 3: train err 0.02, test err 0.02\n",
      "class 4: train err 0.02, test err 0.03\n",
      "class 5: train err 0.02, test err 0.01\n",
      "class 6: train err 0.02, test err 0.05\n",
      "class 7: train err 0.02, test err 0.01\n",
      "class 8: train err 0.02, test err 0.01\n",
      "class 9: train err 0.02, test err 0.01\n",
      "weighted error: 0.1562828011882037\n",
      "test error: 0.0703\n",
      "\n",
      "EPOCH: 8 PROBS: 0.10 0.10 0.10 0.10 0.10 0.10 0.09 0.10 0.10 0.10 \n",
      "class 0: train err 0.00, test err 0.03\n",
      "class 1: train err 0.02, test err 0.00\n",
      "class 2: train err 0.02, test err 0.03\n",
      "class 3: train err 0.02, test err 0.02\n",
      "class 4: train err 0.02, test err 0.03\n",
      "class 5: train err 0.02, test err 0.00\n",
      "class 6: train err 0.02, test err 0.06\n",
      "class 7: train err 0.02, test err 0.01\n",
      "class 8: train err 0.02, test err 0.00\n",
      "class 9: train err 0.02, test err 0.01\n",
      "weighted error: 0.1546662154321324\n",
      "test error: 0.0708\n",
      "\n",
      "EPOCH: 9 PROBS: 0.10 0.10 0.10 0.10 0.10 0.10 0.09 0.10 0.10 0.10 \n",
      "class 0: train err 0.00, test err 0.03\n",
      "class 1: train err 0.02, test err 0.00\n",
      "class 2: train err 0.02, test err 0.03\n",
      "class 3: train err 0.02, test err 0.02\n",
      "class 4: train err 0.02, test err 0.03\n",
      "class 5: train err 0.02, test err 0.01\n",
      "class 6: train err 0.02, test err 0.05\n",
      "class 7: train err 0.02, test err 0.01\n",
      "class 8: train err 0.02, test err 0.00\n",
      "class 9: train err 0.02, test err 0.01\n",
      "weighted error: 0.15353260185053502\n",
      "test error: 0.0699\n"
     ]
    }
   ],
   "source": [
    "boost(data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
