{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dqi/.conda/envs/py36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/home/dqi/.conda/envs/py36/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.svm import LinearSVC\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline functions\n",
    "Helpful for checking that standard classification has high accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def baseline_loss(features, classes):\n",
    "    return tf.losses.sparse_softmax_cross_entropy(classes, features)\n",
    "    \n",
    "def baseline_acc(features, classes):\n",
    "    predictions = tf.argmax(features, axis=1)\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(predictions, classes), tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pair_split(features, classes):\n",
    "    f1, f2 = tf.split(features, 2)\n",
    "    c1, c2 = tf.split(classes, 2)\n",
    "    return f1, f2, c1, c2\n",
    "\n",
    "def pair_hinge_loss(features, classes):\n",
    "    f1, f2, c1, c2 = pair_split(features, classes)\n",
    "    inner_products = tf.reduce_sum(tf.multiply(f1, f2), axis=1)\n",
    "    similarities = tf.sign(tf.cast(tf.equal(c1, c2), tf.float32) - 0.5)\n",
    "    scores = tf.multiply(similarities, inner_products)\n",
    "    return tf.reduce_mean(tf.maximum(1.0 - scores, 0))\n",
    "    \n",
    "def pair_log_loss(features, classes):\n",
    "    f1, f2, c1, c2 = pair_split(features, classes)\n",
    "    inner_products = tf.reduce_sum(tf.multiply(f1, f2), axis=1)\n",
    "    similarities = tf.sign(tf.cast(tf.equal(c1, c2), tf.float32) - 0.5)\n",
    "    scores = tf.multiply(similarities, inner_products)\n",
    "    return tf.reduce_mean(tf.log1p(scores))\n",
    "\n",
    "def pair_acc(features, classes):\n",
    "    f1, f2, c1, c2 = pair_split(features, classes)\n",
    "    predictions = tf.sign(tf.reduce_sum(tf.multiply(f1, f2), axis=1))\n",
    "    similarities = tf.sign(tf.cast(tf.equal(c1, c2), tf.float32) - 0.5)\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(predictions, similarities), tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triplet functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def triplet_hinge_loss(features, classes):\n",
    "    f1, f2, f3 = tf.split(features, 3)\n",
    "    inner_products = tf.reduce_sum(tf.multiply(f1, f2 - f3), axis=1)\n",
    "    return tf.reduce_mean(tf.maximum(1.0 - inner_products, 0))\n",
    "\n",
    "def triplet_log_loss(features, classes):\n",
    "    f1, f2, f3 = tf.split(features, 3)\n",
    "    inner_products = tf.reduce_sum(tf.multiply(f1, f2 - f3), axis=1)\n",
    "    return tf.reduce_mean(tf.log1p(-inner_products))\n",
    "\n",
    "def triplet_acc(features, classes):\n",
    "    f1, f2, f3 = tf.split(features, 3)\n",
    "    inner_products = tf.reduce_sum(tf.multiply(f1, f2 - f3), axis=1)    \n",
    "    return tf.reduce_mean(tf.cast(tf.greater(inner_products, 0), tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(name, l_input, w, b, s):\n",
    "    return tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(l_input, w, strides=[1, s, s, 1], padding='SAME'),b), name=name)\n",
    "\n",
    "def max_pool(name, l_input, k):\n",
    "    return tf.nn.max_pool(l_input, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME', name=name)\n",
    "\n",
    "def norm(name, l_input, lsize=4):\n",
    "    return tf.nn.lrn(l_input, lsize, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name=name)\n",
    "\n",
    "def fc_batch_relu(x, W, b):\n",
    "    return tf.nn.relu(tf.layers.batch_normalization(tf.matmul(x, W) + b))\n",
    "\n",
    "def reg_loss_fn(W):\n",
    "    return tf.nn.l2_loss(W['wd1']) + tf.nn.l2_loss(W['wd2']) + tf.nn.l2_loss(W['out'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ConvModel(object):\n",
    "    def __init__(self, x, y, num_features, loss_fn, acc_fn, lr, reg, dropout):\n",
    "        \"\"\" init the model with hyper-parameters etc \"\"\"\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.num_features = num_features\n",
    "        self.loss_fn = loss_fn\n",
    "        self.acc_fn = acc_fn\n",
    "        self.dropout = dropout        \n",
    "        \n",
    "        initializer = tf.contrib.layers.xavier_initializer(uniform=False)\n",
    "        self.weights = {\n",
    "            'wc1': tf.Variable(initializer([3, 3, 1, 16])),\n",
    "            'wc2': tf.Variable(initializer([3, 3, 16, 32])),\n",
    "            'wc3': tf.Variable(initializer([3, 3, 32, 64])),\n",
    "            'wd1': tf.Variable(initializer([7*7*64, 1024])),\n",
    "            'wd2': tf.Variable(initializer([1024, 128])),\n",
    "            'out': tf.Variable(initializer([128, num_features]))\n",
    "        }\n",
    "        self.biases = {\n",
    "            'bc1': tf.Variable(initializer([16])),\n",
    "            'bc2': tf.Variable(initializer([32])),\n",
    "            'bc3': tf.Variable(initializer([64])),\n",
    "            'bd1': tf.Variable(initializer([1024])),\n",
    "            'bd2': tf.Variable(initializer([128])),\n",
    "            'out': tf.Variable(initializer([num_features]))\n",
    "        }\n",
    "        \n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            self.optimizer = tf.train.AdamOptimizer(lr)\n",
    "            \n",
    "        self.features = self.feature_model()\n",
    "        self.acc = acc_fn(self.features, self.y)\n",
    "        self.loss = loss_fn(self.features, self.y) + reg * reg_loss_fn(self.weights)\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            self.optimize = tf.train.AdamOptimizer(lr).minimize(self.loss)\n",
    "\n",
    "    def feature_model(self):\n",
    "        # Reshape input picture\n",
    "        input = tf.reshape(self.x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "        conv1 = conv2d('conv1', input, self.weights['wc1'], self.biases['bc1'], 1)\n",
    "        conv1 = tf.nn.dropout(conv1, self.dropout)\n",
    "\n",
    "        conv2 = conv2d('conv2', conv1, self.weights['wc2'], self.biases['bc2'], 1)\n",
    "        pool2 = max_pool('pool2', conv2, k=2)\n",
    "        pool2 = tf.nn.dropout(pool2, self.dropout)\n",
    "\n",
    "        conv3 = conv2d('conv3', pool2, self.weights['wc3'], self.biases['bc3'], 1)\n",
    "        pool3 = max_pool('pool3', conv3, k=2)\n",
    "        pool3 = tf.nn.dropout(pool3, self.dropout)\n",
    "\n",
    "        dense1 = tf.reshape(pool3, [-1, self.weights['wd1'].get_shape().as_list()[0]])\n",
    "        dense1 = fc_batch_relu(dense1, self.weights['wd1'], self.biases['bd1'])\n",
    "        dense2 = fc_batch_relu(dense1, self.weights['wd2'], self.biases['bd2'])\n",
    "\n",
    "        out = tf.matmul(dense2, self.weights['out']) + self.biases['out']\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ConvModelSmall(object):\n",
    "    def __init__(self, x, y, num_features, loss_fn, acc_fn, lr, reg, dropout):\n",
    "        \"\"\" init the model with hyper-parameters etc \"\"\"\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.num_features = num_features\n",
    "        self.loss_fn = loss_fn\n",
    "        self.acc_fn = acc_fn\n",
    "        self.dropout = dropout        \n",
    "        \n",
    "        initializer = tf.contrib.layers.xavier_initializer(uniform=False)\n",
    "        self.weights = {\n",
    "            'wc1': tf.Variable(initializer([3, 3, 1, 16])),\n",
    "            'wc2': tf.Variable(initializer([3, 3, 16, 32])),\n",
    "            'wd1': tf.Variable(initializer([7*7*32, 512])),\n",
    "            'wd2': tf.Variable(initializer([512, 128])),\n",
    "            'out': tf.Variable(initializer([128, num_features]))\n",
    "        }\n",
    "        self.biases = {\n",
    "            'bc1': tf.Variable(initializer([16])),\n",
    "            'bc2': tf.Variable(initializer([32])),\n",
    "            'bd1': tf.Variable(initializer([512])),\n",
    "            'bd2': tf.Variable(initializer([128])),\n",
    "            'out': tf.Variable(initializer([num_features]))\n",
    "        }\n",
    "        \n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            self.optimizer = tf.train.AdamOptimizer(lr)\n",
    "            \n",
    "        self.features = self.feature_model()\n",
    "        self.acc = acc_fn(self.features, self.y)\n",
    "        self.loss = loss_fn(self.features, self.y) + reg * reg_loss_fn(self.weights)\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            self.optimize = tf.train.AdamOptimizer(lr).minimize(self.loss)\n",
    "\n",
    "    def feature_model(self):\n",
    "        # Reshape input picture\n",
    "        input = tf.reshape(self.x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "        conv1 = conv2d('conv1', input, self.weights['wc1'], self.biases['bc1'], 2)\n",
    "        conv1 = tf.nn.dropout(conv1, self.dropout)\n",
    "        conv2 = conv2d('conv2', conv1, self.weights['wc2'], self.biases['bc2'], 2)\n",
    "        conv2 = tf.nn.dropout(conv2, self.dropout)\n",
    "        dense1 = tf.reshape(conv2, [-1, self.weights['wd1'].get_shape().as_list()[0]])\n",
    "        dense1 = fc_batch_relu(dense1, self.weights['wd1'], self.biases['bd1'])\n",
    "        dense2 = fc_batch_relu(dense1, self.weights['wd2'], self.biases['bd2'])\n",
    "\n",
    "        out = tf.matmul(dense2, self.weights['out']) + self.biases['out']\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ConvModelSmaller(object):\n",
    "    def __init__(self, x, y, num_features, loss_fn, acc_fn, lr, reg, dropout):\n",
    "        \"\"\" init the model with hyper-parameters etc \"\"\"\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.num_features = num_features\n",
    "        self.loss_fn = loss_fn\n",
    "        self.acc_fn = acc_fn\n",
    "        self.dropout = dropout        \n",
    "        \n",
    "        initializer = tf.contrib.layers.xavier_initializer(uniform=False)\n",
    "        self.weights = {\n",
    "            'wc1': tf.Variable(initializer([3, 3, 1, 16])),\n",
    "            'wc2': tf.Variable(initializer([3, 3, 16, 16])),\n",
    "            'wc3': tf.Variable(initializer([3, 3, 16, 32])),\n",
    "            'wd1': tf.Variable(initializer([4*4*32, 256])),\n",
    "            'wd2': tf.Variable(initializer([256, 64])),\n",
    "            'out': tf.Variable(initializer([64, num_features]))\n",
    "        }\n",
    "        self.biases = {\n",
    "            'bc1': tf.Variable(initializer([16])),\n",
    "            'bc2': tf.Variable(initializer([16])),\n",
    "            'bc3': tf.Variable(initializer([32])),\n",
    "            'bd1': tf.Variable(initializer([256])),\n",
    "            'bd2': tf.Variable(initializer([64])),\n",
    "            'out': tf.Variable(initializer([num_features]))\n",
    "        }\n",
    "        \n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            self.optimizer = tf.train.AdamOptimizer(lr)\n",
    "            \n",
    "        self.features = self.feature_model()\n",
    "        self.acc = acc_fn(self.features, self.y)\n",
    "        self.loss = loss_fn(self.features, self.y) + reg * reg_loss_fn(self.weights)\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            self.optimize = tf.train.AdamOptimizer(lr).minimize(self.loss)\n",
    "\n",
    "    def feature_model(self):\n",
    "        # Reshape input picture\n",
    "        input = tf.reshape(self.x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "        conv1 = conv2d('conv1', input, self.weights['wc1'], self.biases['bc1'], 2)\n",
    "        conv1 = tf.nn.dropout(conv1, self.dropout)\n",
    "        conv2 = conv2d('conv2', conv1, self.weights['wc2'], self.biases['bc2'], 2)\n",
    "        conv2 = tf.nn.dropout(conv2, self.dropout)\n",
    "        conv3 = conv2d('conv3', conv2, self.weights['wc3'], self.biases['bc3'], 2)\n",
    "        conv3 = tf.nn.dropout(conv3, self.dropout)\n",
    "\n",
    "        dense1 = tf.reshape(conv3, [-1, self.weights['wd1'].get_shape().as_list()[0]])\n",
    "        dense1 = fc_batch_relu(dense1, self.weights['wd1'], self.biases['bd1'])\n",
    "        dense2 = fc_batch_relu(dense1, self.weights['wd2'], self.biases['bd2'])\n",
    "\n",
    "        out = tf.matmul(dense2, self.weights['out']) + self.biases['out']\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_model(model, loss_fn, acc_fn, num_features, num_steps, lr, keep_prob, reg):\n",
    "\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "    y = tf.placeholder(tf.int64, shape=[None])\n",
    "    dropout = tf.placeholder(tf.float32)\n",
    "    model = model(x, y, num_features, loss_fn, acc_fn, lr, reg, dropout)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        print(\"begin training // num_features: %g, lr: %g, reg: %g\"%(num_features, lr, reg))\n",
    "        train_time = time.time()\n",
    "        for step in range(num_steps):\n",
    "            x_, y_ = mnist.train.next_batch(200)     \n",
    "            sess.run(model.optimize, feed_dict={x:x_, y:y_, dropout:keep_prob})\n",
    "            \n",
    "            if step % 1000 == 0:\n",
    "                train_loss = sess.run(model.loss, feed_dict={x:x_, y:y_, dropout: 1.0})                \n",
    "                train_error = 1 - sess.run(model.acc, feed_dict={x:x_, y:y_, dropout: 1.0})\n",
    "                \n",
    "                x_, y_ = mnist.test.next_batch(1000)\n",
    "                test_loss = sess.run(model.loss, feed_dict={x:x_, y:y_, dropout: 1.0})                \n",
    "                test_error = 1 - sess.run(model.acc, feed_dict={x:x_, y:y_, dropout: 1.0})\n",
    "                print(\"\\tstep %d: train loss %g, train error %g, test loss %g, test error %g\"%\n",
    "                      (step, train_loss, train_error, test_loss, test_error))  \n",
    "                \n",
    "                svc = LinearSVC(random_state=0)\n",
    "                f1 = sess.run(model.features, feed_dict={x:mnist.train.images, y:mnist.train.labels, dropout: 1.0}) \n",
    "                svc.fit(f1, mnist.train.labels)\n",
    "                f2 = sess.run(model.features, feed_dict={x:mnist.test.images, y:mnist.test.labels, dropout: 1.0})\n",
    "                print(\"\\tclassification accuracy: {:.4f}\".format(svc.score(f2, mnist.test.labels)))\n",
    "        train_time = time.time() - train_time\n",
    "        print(\"end training // time elapsed: %.4f s\"%(train_time))\n",
    "        \n",
    "        eval_test_time = time.time()\n",
    "        x_, y_ = mnist.test.next_batch(1000)\n",
    "        test_error = 1 - sess.run(model.acc, feed_dict={x:x_, y:y_, dropout:1.0})\n",
    "        eval_test_time = time.time() - eval_test_time\n",
    "        print(\"test set error: %.4f // time elapsed: %.4f s\"%(test_error, eval_test_time))\n",
    "        \n",
    "        svc = LinearSVC(random_state=0)\n",
    "        f1 = sess.run(model.features, feed_dict={x:mnist.train.images, y:mnist.train.labels, dropout: 1.0}) \n",
    "        svc.fit(f1, mnist.train.labels)\n",
    "        f2 = sess.run(model.features, feed_dict={x:mnist.test.images, y:mnist.test.labels, dropout: 1.0})\n",
    "        print(\"classification accuracy: {:.4f}\".format(svc.score(f2, mnist.test.labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Current experiments (uniform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin training // num_features: 10, lr: 0.0001, reg: 0\n",
      "\tstep 0: train loss 3.07616, train error 0.88, test loss 3.18369, test error 0.91\n",
      "\tclassification accuracy: 0.4628\n",
      "\tstep 1000: train loss 1.00625, train error 0.92, test loss 1.00607, test error 0.926\n",
      "\tclassification accuracy: 0.4631\n",
      "\tstep 2000: train loss 1.00508, train error 0.8, test loss 1.00558, test error 0.818\n",
      "\tclassification accuracy: 0.5554\n",
      "\tstep 3000: train loss 1.04824, train error 0.53, test loss 0.89831, test error 0.42\n",
      "\tclassification accuracy: 0.7421\n",
      "\tstep 4000: train loss 0.74638, train error 0.3, test loss 0.776152, test error 0.332\n",
      "\tclassification accuracy: 0.7948\n",
      "\tstep 5000: train loss 0.63737, train error 0.23, test loss 0.590983, test error 0.198\n",
      "\tclassification accuracy: 0.8082\n",
      "\tstep 6000: train loss 0.701635, train error 0.2, test loss 0.620889, test error 0.194\n",
      "\tclassification accuracy: 0.8058\n",
      "\tstep 7000: train loss 0.634198, train error 0.23, test loss 0.53412, test error 0.168\n",
      "\tclassification accuracy: 0.8158\n",
      "\tstep 8000: train loss 0.286382, train error 0.05, test loss 0.450619, test error 0.118\n",
      "\tclassification accuracy: 0.8323\n",
      "\tstep 9000: train loss 0.263923, train error 0.1, test loss 0.418996, test error 0.126\n",
      "\tclassification accuracy: 0.8214\n",
      "\tstep 10000: train loss 0.484394, train error 0.16, test loss 0.475326, test error 0.122\n",
      "\tclassification accuracy: 0.8271\n",
      "\tstep 11000: train loss 0.301883, train error 0.12, test loss 0.515701, test error 0.13\n",
      "\tclassification accuracy: 0.8353\n",
      "\tstep 12000: train loss 0.356598, train error 0.11, test loss 0.304392, test error 0.11\n",
      "\tclassification accuracy: 0.8462\n",
      "\tstep 13000: train loss 0.305922, train error 0.12, test loss 0.38267, test error 0.128\n",
      "\tclassification accuracy: 0.8601\n",
      "\tstep 14000: train loss 0.312146, train error 0.13, test loss 0.37611, test error 0.11\n",
      "\tclassification accuracy: 0.8738\n",
      "\tstep 15000: train loss 0.329969, train error 0.11, test loss 0.261051, test error 0.098\n",
      "\tclassification accuracy: 0.8863\n",
      "\tstep 16000: train loss 0.192272, train error 0.05, test loss 0.333619, test error 0.096\n",
      "\tclassification accuracy: 0.8940\n",
      "\tstep 17000: train loss 0.665243, train error 0.19, test loss 0.24658, test error 0.086\n",
      "\tclassification accuracy: 0.9039\n",
      "\tstep 18000: train loss 0.328317, train error 0.1, test loss 0.263864, test error 0.092\n",
      "\tclassification accuracy: 0.9186\n",
      "\tstep 19000: train loss 0.255933, train error 0.1, test loss 0.388353, test error 0.118\n",
      "\tclassification accuracy: 0.9264\n",
      "end training // time elapsed: 566.0914 s\n",
      "test set error: 0.0740 // time elapsed: 0.0280 s\n",
      "classification accuracy: 0.9309\n"
     ]
    }
   ],
   "source": [
    "test_model(model=ConvModelSmaller, loss_fn=pair_hinge_loss, acc_fn=pair_acc, \n",
    "           num_features=10, num_steps=20000, lr=0.0001, keep_prob=0.8, reg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin training // num_features: 20, lr: 0.0001, reg: 0\n",
      "\tstep 0: train loss 2.70419, train error 0.9, test loss 2.58948, test error 0.864\n",
      "\tclassification accuracy: 0.6301\n",
      "\tstep 1000: train loss 1.01106, train error 0.86, test loss 1.01179, test error 0.91\n",
      "\tclassification accuracy: 0.6387\n",
      "\tstep 2000: train loss 1.00765, train error 0.89, test loss 1.00763, test error 0.888\n",
      "\tclassification accuracy: 0.6924\n",
      "\tstep 3000: train loss 0.997378, train error 0.52, test loss 0.916495, test error 0.444\n",
      "\tclassification accuracy: 0.7816\n",
      "\tstep 4000: train loss 0.756259, train error 0.28, test loss 0.853314, test error 0.384\n",
      "\tclassification accuracy: 0.7976\n",
      "\tstep 5000: train loss 0.776865, train error 0.29, test loss 0.620479, test error 0.246\n",
      "\tclassification accuracy: 0.8369\n",
      "\tstep 6000: train loss 0.541543, train error 0.21, test loss 0.64115, test error 0.226\n",
      "\tclassification accuracy: 0.8626\n",
      "\tstep 7000: train loss 0.518648, train error 0.14, test loss 0.563742, test error 0.22\n",
      "\tclassification accuracy: 0.8762\n",
      "\tstep 8000: train loss 0.451762, train error 0.14, test loss 0.571582, test error 0.184\n",
      "\tclassification accuracy: 0.8764\n",
      "\tstep 9000: train loss 0.955617, train error 0.16, test loss 0.372617, test error 0.118\n",
      "\tclassification accuracy: 0.8775\n",
      "\tstep 10000: train loss 0.455929, train error 0.12, test loss 0.327056, test error 0.106\n",
      "\tclassification accuracy: 0.8860\n",
      "\tstep 11000: train loss 0.213723, train error 0.11, test loss 0.383147, test error 0.13\n",
      "\tclassification accuracy: 0.8936\n",
      "\tstep 12000: train loss 0.297161, train error 0.05, test loss 0.3039, test error 0.104\n",
      "\tclassification accuracy: 0.9009\n",
      "\tstep 13000: train loss 0.255625, train error 0.07, test loss 0.238342, test error 0.082\n",
      "\tclassification accuracy: 0.9084\n",
      "\tstep 14000: train loss 0.163075, train error 0.05, test loss 0.290743, test error 0.076\n",
      "\tclassification accuracy: 0.9157\n",
      "\tstep 15000: train loss 0.26193, train error 0.1, test loss 0.346333, test error 0.096\n",
      "\tclassification accuracy: 0.9230\n",
      "\tstep 16000: train loss 0.187122, train error 0.09, test loss 0.433868, test error 0.072\n",
      "\tclassification accuracy: 0.9294\n",
      "\tstep 17000: train loss 0.182063, train error 0.05, test loss 0.200707, test error 0.072\n",
      "\tclassification accuracy: 0.9329\n",
      "\tstep 18000: train loss 0.245165, train error 0.05, test loss 0.263033, test error 0.058\n",
      "\tclassification accuracy: 0.9402\n",
      "\tstep 19000: train loss 0.157796, train error 0.06, test loss 0.180162, test error 0.064\n",
      "\tclassification accuracy: 0.9445\n",
      "end training // time elapsed: 493.7874 s\n",
      "test set error: 0.0380 // time elapsed: 0.0144 s\n",
      "classification accuracy: 0.9492\n"
     ]
    }
   ],
   "source": [
    "test_model(model=ConvModelSmaller, loss_fn=pair_hinge_loss, acc_fn=pair_acc, \n",
    "           num_features=20, num_steps=20000, lr=0.0001, keep_prob=0.8, reg=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConvModelSmaller experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Halving the learning rate isn't helpful. Smaller convnet gets reasonably good results: 0.9146 accuracy on 10 features, 0.9508 on 20 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin training // num_features: 10, lr: 0.0001, reg: 0\n",
      "\tstep 0: train loss 3.13961, train error 0.86, test loss 3.36935, test error 0.92\n",
      "\tclassification accuracy: 0.4942\n",
      "\tstep 1000: train loss 1.00971, train error 0.88, test loss 1.00981, test error 0.894\n",
      "\tclassification accuracy: 0.5608\n",
      "\tstep 2000: train loss 1.00426, train error 0.73, test loss 1.00526, test error 0.714\n",
      "\tclassification accuracy: 0.6266\n",
      "\tstep 3000: train loss 0.920731, train error 0.36, test loss 0.83678, test error 0.366\n",
      "\tclassification accuracy: 0.7437\n",
      "\tstep 4000: train loss 0.601978, train error 0.22, test loss 0.763397, test error 0.316\n",
      "\tclassification accuracy: 0.7906\n",
      "\tstep 5000: train loss 0.761131, train error 0.28, test loss 0.642095, test error 0.248\n",
      "\tclassification accuracy: 0.8248\n",
      "\tstep 6000: train loss 0.464199, train error 0.16, test loss 0.572754, test error 0.204\n",
      "\tclassification accuracy: 0.8437\n",
      "\tstep 7000: train loss 0.649026, train error 0.23, test loss 0.495997, test error 0.168\n",
      "\tclassification accuracy: 0.8489\n",
      "\tstep 8000: train loss 0.597191, train error 0.19, test loss 0.440295, test error 0.152\n",
      "\tclassification accuracy: 0.8577\n",
      "\tstep 9000: train loss 0.581457, train error 0.14, test loss 0.427545, test error 0.154\n",
      "\tclassification accuracy: 0.8624\n",
      "\tstep 10000: train loss 0.492492, train error 0.17, test loss 0.399082, test error 0.128\n",
      "\tclassification accuracy: 0.8689\n",
      "\tstep 11000: train loss 0.345295, train error 0.13, test loss 0.355014, test error 0.122\n",
      "\tclassification accuracy: 0.8731\n",
      "\tstep 12000: train loss 0.241131, train error 0.1, test loss 0.270351, test error 0.09\n",
      "\tclassification accuracy: 0.8761\n",
      "\tstep 13000: train loss 0.280236, train error 0.08, test loss 0.344187, test error 0.09\n",
      "\tclassification accuracy: 0.8834\n",
      "\tstep 14000: train loss 0.284251, train error 0.1, test loss 0.283514, test error 0.104\n",
      "\tclassification accuracy: 0.8846\n",
      "\tstep 15000: train loss 0.630407, train error 0.14, test loss 0.34087, test error 0.098\n",
      "\tclassification accuracy: 0.8914\n",
      "\tstep 16000: train loss 0.261276, train error 0.1, test loss 0.327936, test error 0.092\n",
      "\tclassification accuracy: 0.8966\n",
      "\tstep 17000: train loss 0.168394, train error 0.05, test loss 0.246749, test error 0.082\n",
      "\tclassification accuracy: 0.8997\n",
      "\tstep 18000: train loss 0.227177, train error 0.07, test loss 0.431705, test error 0.104\n",
      "\tclassification accuracy: 0.9050\n",
      "\tstep 19000: train loss 0.101065, train error 0.03, test loss 0.341949, test error 0.094\n",
      "\tclassification accuracy: 0.9100\n",
      "end training // time elapsed: 465.5294 s\n",
      "test set error: 0.0680 // time elapsed: 0.0265 s\n",
      "classification accuracy: 0.9146\n"
     ]
    }
   ],
   "source": [
    "test_model(model=ConvModelSmaller, loss_fn=pair_hinge_loss, acc_fn=pair_acc, \n",
    "           num_features=10, num_steps=20000, lr=0.0001, keep_prob=0.8, reg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin training // num_features: 10, lr: 5e-05, reg: 0\n",
      "\tstep 0: train loss 2.18399, train error 0.87, test loss 2.20302, test error 0.878\n",
      "\tclassification accuracy: 0.5234\n",
      "\tstep 1000: train loss 1.00991, train error 0.89, test loss 1.01056, test error 0.904\n",
      "\tclassification accuracy: 0.5251\n",
      "\tstep 2000: train loss 1.006, train error 0.81, test loss 1.00606, test error 0.812\n",
      "\tclassification accuracy: 0.5679\n",
      "\tstep 3000: train loss 0.997809, train error 0.45, test loss 0.924546, test error 0.424\n",
      "\tclassification accuracy: 0.6992\n",
      "\tstep 4000: train loss 0.831077, train error 0.37, test loss 0.868367, test error 0.426\n",
      "\tclassification accuracy: 0.7383\n",
      "\tstep 5000: train loss 0.778174, train error 0.34, test loss 0.882081, test error 0.374\n",
      "\tclassification accuracy: 0.7817\n",
      "\tstep 6000: train loss 0.890469, train error 0.34, test loss 0.845914, test error 0.364\n",
      "\tclassification accuracy: 0.8015\n",
      "\tstep 7000: train loss 0.548851, train error 0.22, test loss 0.724886, test error 0.282\n",
      "\tclassification accuracy: 0.8191\n",
      "\tstep 8000: train loss 0.641516, train error 0.26, test loss 0.697257, test error 0.258\n",
      "\tclassification accuracy: 0.8199\n",
      "\tstep 9000: train loss 0.555038, train error 0.22, test loss 0.640695, test error 0.238\n",
      "\tclassification accuracy: 0.8293\n",
      "\tstep 10000: train loss 0.653701, train error 0.27, test loss 0.616803, test error 0.212\n",
      "\tclassification accuracy: 0.8355\n",
      "\tstep 11000: train loss 0.389949, train error 0.16, test loss 0.509076, test error 0.202\n",
      "\tclassification accuracy: 0.8395\n",
      "\tstep 12000: train loss 0.473025, train error 0.14, test loss 0.500676, test error 0.18\n",
      "\tclassification accuracy: 0.8438\n",
      "\tstep 13000: train loss 0.462488, train error 0.17, test loss 0.445566, test error 0.158\n",
      "\tclassification accuracy: 0.8436\n",
      "\tstep 14000: train loss 0.416978, train error 0.12, test loss 0.413651, test error 0.132\n",
      "\tclassification accuracy: 0.8473\n",
      "\tstep 15000: train loss 0.296827, train error 0.12, test loss 0.430596, test error 0.132\n",
      "\tclassification accuracy: 0.8461\n",
      "\tstep 16000: train loss 0.361963, train error 0.12, test loss 0.430723, test error 0.118\n",
      "\tclassification accuracy: 0.8413\n",
      "\tstep 17000: train loss 0.505184, train error 0.21, test loss 0.397434, test error 0.134\n",
      "\tclassification accuracy: 0.8451\n",
      "\tstep 18000: train loss 0.385568, train error 0.15, test loss 0.371016, test error 0.134\n",
      "\tclassification accuracy: 0.8400\n",
      "\tstep 19000: train loss 0.440018, train error 0.11, test loss 0.292403, test error 0.094\n",
      "\tclassification accuracy: 0.8457\n",
      "end training // time elapsed: 457.3147 s\n",
      "test set error: 0.1220 // time elapsed: 0.0121 s\n",
      "classification accuracy: 0.8523\n"
     ]
    }
   ],
   "source": [
    "test_model(model=ConvModelSmaller, loss_fn=pair_hinge_loss, acc_fn=pair_acc, \n",
    "           num_features=10, num_steps=20000, lr=0.00005, keep_prob=0.8, reg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin training // num_features: 20, lr: 0.0001, reg: 0\n",
      "\tstep 0: train loss 1.88371, train error 0.9, test loss 1.82962, test error 0.878\n",
      "\tclassification accuracy: 0.5797\n",
      "\tstep 1000: train loss 1.01715, train error 0.92, test loss 1.01732, test error 0.906\n",
      "\tclassification accuracy: 0.6039\n",
      "\tstep 2000: train loss 0.903325, train error 0.4, test loss 0.918699, test error 0.48\n",
      "\tclassification accuracy: 0.7637\n",
      "\tstep 3000: train loss 0.688052, train error 0.29, test loss 0.802041, test error 0.336\n",
      "\tclassification accuracy: 0.8151\n",
      "\tstep 4000: train loss 0.692119, train error 0.29, test loss 0.570496, test error 0.244\n",
      "\tclassification accuracy: 0.8355\n",
      "\tstep 5000: train loss 0.418868, train error 0.17, test loss 0.575287, test error 0.212\n",
      "\tclassification accuracy: 0.8603\n",
      "\tstep 6000: train loss 0.43912, train error 0.19, test loss 0.541096, test error 0.148\n",
      "\tclassification accuracy: 0.8616\n",
      "\tstep 7000: train loss 0.335555, train error 0.1, test loss 0.42836, test error 0.152\n",
      "\tclassification accuracy: 0.8638\n",
      "\tstep 8000: train loss 0.354523, train error 0.09, test loss 0.371737, test error 0.102\n",
      "\tclassification accuracy: 0.8746\n",
      "\tstep 9000: train loss 0.273217, train error 0.11, test loss 0.343277, test error 0.096\n",
      "\tclassification accuracy: 0.8909\n",
      "\tstep 10000: train loss 0.324597, train error 0.12, test loss 0.322665, test error 0.108\n",
      "\tclassification accuracy: 0.8904\n",
      "\tstep 11000: train loss 0.317982, train error 0.1, test loss 0.314264, test error 0.088\n",
      "\tclassification accuracy: 0.9026\n",
      "\tstep 12000: train loss 0.247336, train error 0.08, test loss 0.265365, test error 0.07\n",
      "\tclassification accuracy: 0.9045\n",
      "\tstep 13000: train loss 0.525893, train error 0.11, test loss 0.217875, test error 0.068\n",
      "\tclassification accuracy: 0.9153\n",
      "\tstep 14000: train loss 0.332716, train error 0.13, test loss 0.194617, test error 0.06\n",
      "\tclassification accuracy: 0.9217\n",
      "\tstep 15000: train loss 0.125059, train error 0.04, test loss 0.176347, test error 0.058\n",
      "\tclassification accuracy: 0.9241\n",
      "\tstep 16000: train loss 0.353419, train error 0.08, test loss 0.139239, test error 0.04\n",
      "\tclassification accuracy: 0.9294\n",
      "\tstep 17000: train loss 0.0943622, train error 0.04, test loss 0.129586, test error 0.05\n",
      "\tclassification accuracy: 0.9368\n",
      "\tstep 18000: train loss 0.298359, train error 0.06, test loss 0.174875, test error 0.052\n",
      "\tclassification accuracy: 0.9415\n",
      "\tstep 19000: train loss 0.182699, train error 0.08, test loss 0.298519, test error 0.058\n",
      "\tclassification accuracy: 0.9464\n",
      "end training // time elapsed: 493.0842 s\n",
      "test set error: 0.0520 // time elapsed: 0.0139 s\n",
      "classification accuracy: 0.9508\n"
     ]
    }
   ],
   "source": [
    "test_model(model=ConvModelSmaller, loss_fn=pair_hinge_loss, acc_fn=pair_acc, \n",
    "           num_features=20, num_steps=20000, lr=0.0001, keep_prob=0.8, reg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin training // num_features: 20, lr: 5e-05, reg: 0\n",
      "\tstep 0: train loss 2.1085, train error 0.93, test loss 2.05052, test error 0.902\n",
      "\tclassification accuracy: 0.5347\n",
      "\tstep 1000: train loss 1.0293, train error 0.95, test loss 1.0279, test error 0.916\n",
      "\tclassification accuracy: 0.5777\n",
      "\tstep 2000: train loss 1.02165, train error 0.89, test loss 1.02285, test error 0.904\n",
      "\tclassification accuracy: 0.5938\n"
     ]
    }
   ],
   "source": [
    "test_model(model=ConvModelSmaller, loss_fn=pair_hinge_loss, acc_fn=pair_acc, \n",
    "           num_features=20, num_steps=20000, lr=0.00005, keep_prob=0.8, reg=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConvModelSmall experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that dropout rates matter much less. With a smaller model, we also seem to learn more quickly or learn better.\n",
    "Keep prob: 0.9\n",
    "    20 features: 0.9785, 0.0100\n",
    "    10features: 0.9754, 0.0280\n",
    "Keep prob: 0.8\n",
    "    10 features: 0.9559, 0.0380\n",
    "    20 features: 0.9783, 0.0240"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin training\n",
      "\tstep 0: train loss 1.43393, train error 0.89, test loss 1.42729, test error 0.886\n",
      "\tclassification accuracy: 0.4344\n",
      "\tstep 1000: train loss 0.765165, train error 0.31, test loss 0.861398, test error 0.364\n",
      "\tclassification accuracy: 0.7969\n",
      "\tstep 2000: train loss 0.761658, train error 0.27, test loss 0.636762, test error 0.22\n",
      "\tclassification accuracy: 0.8368\n",
      "\tstep 3000: train loss 0.383273, train error 0.16, test loss 0.48594, test error 0.164\n",
      "\tclassification accuracy: 0.8262\n",
      "\tstep 4000: train loss 0.486939, train error 0.18, test loss 0.428506, test error 0.15\n",
      "\tclassification accuracy: 0.8262\n",
      "\tstep 5000: train loss 0.386481, train error 0.09, test loss 0.341318, test error 0.106\n",
      "\tclassification accuracy: 0.8561\n",
      "\tstep 6000: train loss 0.272224, train error 0.07, test loss 0.474743, test error 0.1\n",
      "\tclassification accuracy: 0.8584\n",
      "\tstep 7000: train loss 0.247179, train error 0.1, test loss 0.330533, test error 0.1\n",
      "\tclassification accuracy: 0.8831\n",
      "\tstep 8000: train loss 0.239222, train error 0.08, test loss 0.257587, test error 0.09\n",
      "\tclassification accuracy: 0.9032\n",
      "\tstep 9000: train loss 0.288984, train error 0.08, test loss 0.246069, test error 0.066\n",
      "\tclassification accuracy: 0.9167\n",
      "\tstep 10000: train loss 0.0965765, train error 0.02, test loss 0.230068, test error 0.084\n",
      "\tclassification accuracy: 0.9241\n",
      "\tstep 11000: train loss 0.346532, train error 0.12, test loss 0.196833, test error 0.068\n",
      "\tclassification accuracy: 0.9338\n",
      "\tstep 12000: train loss 0.204862, train error 0.09, test loss 0.230871, test error 0.088\n",
      "\tclassification accuracy: 0.9378\n",
      "\tstep 13000: train loss 0.0709239, train error 0.02, test loss 0.24276, test error 0.054\n",
      "\tclassification accuracy: 0.9460\n",
      "\tstep 14000: train loss 0.201126, train error 0.07, test loss 0.164633, test error 0.044\n",
      "\tclassification accuracy: 0.9552\n",
      "\tstep 15000: train loss 0.0759277, train error 0.04, test loss 0.320039, test error 0.042\n",
      "\tclassification accuracy: 0.9574\n",
      "\tstep 16000: train loss 0.12142, train error 0.04, test loss 0.124373, test error 0.04\n",
      "\tclassification accuracy: 0.9637\n",
      "\tstep 17000: train loss 0.158654, train error 0.04, test loss 0.180423, test error 0.048\n",
      "\tclassification accuracy: 0.9665\n",
      "\tstep 18000: train loss 0.0381056, train error 0.00999999, test loss 0.0672389, test error 0.02\n",
      "\tclassification accuracy: 0.9713\n",
      "\tstep 19000: train loss 0.0622269, train error 0, test loss 0.181757, test error 0.026\n",
      "\tclassification accuracy: 0.9736\n",
      "end training // time elapsed: 531.7787 s\n",
      "test set error: 0.0280 // time elapsed: 0.0188 s\n",
      "classification accuracy: 0.9754\n"
     ]
    }
   ],
   "source": [
    "test_model(model=ConvModelSmall, loss_fn=pair_hinge_loss, acc_fn=pair_acc, \n",
    "           num_features=10, num_steps=20000, lr=0.0001, keep_prob=0.9, reg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin training\n",
      "\tstep 0: train loss 2.45935, train error 0.9, test loss 2.44679, test error 0.89\n",
      "\tclassification accuracy: 0.6239\n",
      "\tstep 1000: train loss 0.822141, train error 0.25, test loss 0.701273, test error 0.27\n",
      "\tclassification accuracy: 0.8474\n",
      "\tstep 2000: train loss 0.536236, train error 0.16, test loss 0.559991, test error 0.176\n",
      "\tclassification accuracy: 0.8884\n",
      "\tstep 3000: train loss 0.451835, train error 0.13, test loss 0.387406, test error 0.116\n",
      "\tclassification accuracy: 0.8964\n",
      "\tstep 4000: train loss 0.287364, train error 0.09, test loss 0.384703, test error 0.128\n",
      "\tclassification accuracy: 0.9106\n",
      "\tstep 5000: train loss 0.351404, train error 0.09, test loss 0.254328, test error 0.078\n",
      "\tclassification accuracy: 0.9162\n",
      "\tstep 6000: train loss 0.159584, train error 0.07, test loss 0.254165, test error 0.084\n",
      "\tclassification accuracy: 0.9254\n",
      "\tstep 7000: train loss 0.143335, train error 0.04, test loss 0.201902, test error 0.056\n",
      "\tclassification accuracy: 0.9370\n",
      "\tstep 8000: train loss 0.0706939, train error 0.00999999, test loss 0.180276, test error 0.042\n",
      "\tclassification accuracy: 0.9476\n",
      "\tstep 9000: train loss 0.193754, train error 0.06, test loss 0.309831, test error 0.06\n",
      "\tclassification accuracy: 0.9541\n",
      "\tstep 10000: train loss 0.134558, train error 0.05, test loss 0.159631, test error 0.06\n",
      "\tclassification accuracy: 0.9598\n",
      "\tstep 11000: train loss 0.156878, train error 0.06, test loss 0.231168, test error 0.04\n",
      "\tclassification accuracy: 0.9631\n",
      "\tstep 12000: train loss 0.159678, train error 0.04, test loss 0.0759186, test error 0.024\n",
      "\tclassification accuracy: 0.9676\n",
      "\tstep 13000: train loss 0.0901316, train error 0.03, test loss 0.113066, test error 0.042\n",
      "\tclassification accuracy: 0.9693\n",
      "\tstep 14000: train loss 0.050016, train error 0.00999999, test loss 0.107289, test error 0.038\n",
      "\tclassification accuracy: 0.9724\n",
      "\tstep 15000: train loss 0.0258406, train error 0.00999999, test loss 0.061497, test error 0.02\n",
      "\tclassification accuracy: 0.9741\n",
      "\tstep 16000: train loss 0.0786844, train error 0.02, test loss 0.123227, test error 0.014\n",
      "\tclassification accuracy: 0.9756\n",
      "\tstep 17000: train loss 0.0438471, train error 0.00999999, test loss 0.258203, test error 0.02\n",
      "\tclassification accuracy: 0.9774\n",
      "\tstep 18000: train loss 0.0459223, train error 0.02, test loss 0.057648, test error 0.02\n",
      "\tclassification accuracy: 0.9785\n",
      "\tstep 19000: train loss 0.079483, train error 0.02, test loss 0.113837, test error 0.026\n",
      "\tclassification accuracy: 0.9778\n",
      "end training // time elapsed: 482.2789 s\n",
      "test set error: 0.0100 // time elapsed: 0.0441 s\n",
      "classification accuracy: 0.9785\n"
     ]
    }
   ],
   "source": [
    "test_model(model=ConvModelSmall, loss_fn=pair_hinge_loss, acc_fn=pair_acc, \n",
    "           num_features=20, num_steps=20000, lr=0.0001, keep_prob=0.9, reg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin training\n",
      "\tstep 0: train loss 2.01089, train error 0.9, test loss 1.93519, test error 0.866\n",
      "\tclassification accuracy: 0.4117\n",
      "\tstep 1000: train loss 1.0546, train error 0.53, test loss 0.917702, test error 0.436\n",
      "\tclassification accuracy: 0.6805\n",
      "\tstep 2000: train loss 0.658991, train error 0.24, test loss 0.788234, test error 0.284\n",
      "\tclassification accuracy: 0.8004\n",
      "\tstep 3000: train loss 0.73328, train error 0.28, test loss 0.644832, test error 0.236\n",
      "\tclassification accuracy: 0.8276\n",
      "\tstep 4000: train loss 0.587281, train error 0.17, test loss 0.455392, test error 0.138\n",
      "\tclassification accuracy: 0.8088\n",
      "\tstep 5000: train loss 0.453873, train error 0.17, test loss 0.359427, test error 0.098\n",
      "\tclassification accuracy: 0.8352\n",
      "\tstep 6000: train loss 0.3686, train error 0.15, test loss 0.309451, test error 0.094\n",
      "\tclassification accuracy: 0.8547\n",
      "\tstep 7000: train loss 0.326878, train error 0.12, test loss 0.392441, test error 0.116\n",
      "\tclassification accuracy: 0.8711\n",
      "\tstep 8000: train loss 0.377019, train error 0.12, test loss 0.279578, test error 0.1\n",
      "\tclassification accuracy: 0.8806\n",
      "\tstep 9000: train loss 0.283907, train error 0.1, test loss 0.282183, test error 0.08\n",
      "\tclassification accuracy: 0.8894\n",
      "\tstep 10000: train loss 0.191493, train error 0.04, test loss 0.245998, test error 0.076\n",
      "\tclassification accuracy: 0.9038\n",
      "\tstep 11000: train loss 0.324693, train error 0.1, test loss 0.383037, test error 0.076\n",
      "\tclassification accuracy: 0.9104\n",
      "\tstep 12000: train loss 0.628395, train error 0.1, test loss 0.318541, test error 0.086\n",
      "\tclassification accuracy: 0.9226\n",
      "\tstep 13000: train loss 0.145017, train error 0.05, test loss 0.181276, test error 0.054\n",
      "\tclassification accuracy: 0.9317\n",
      "\tstep 14000: train loss 0.127391, train error 0.03, test loss 0.170036, test error 0.052\n",
      "\tclassification accuracy: 0.9347\n",
      "\tstep 15000: train loss 0.184417, train error 0.07, test loss 0.189529, test error 0.066\n",
      "\tclassification accuracy: 0.9408\n",
      "\tstep 16000: train loss 0.145903, train error 0.05, test loss 0.154344, test error 0.048\n",
      "\tclassification accuracy: 0.9436\n",
      "\tstep 17000: train loss 0.399417, train error 0.08, test loss 0.148992, test error 0.052\n",
      "\tclassification accuracy: 0.9464\n",
      "\tstep 18000: train loss 0.197377, train error 0.07, test loss 0.171395, test error 0.054\n",
      "\tclassification accuracy: 0.9511\n",
      "\tstep 19000: train loss 0.125566, train error 0.04, test loss 0.140163, test error 0.05\n",
      "\tclassification accuracy: 0.9530\n",
      "end training // time elapsed: 546.8201 s\n",
      "test set error: 0.0380 // time elapsed: 0.0203 s\n",
      "classification accuracy: 0.9559\n"
     ]
    }
   ],
   "source": [
    "test_model(model=ConvModelSmall, loss_fn=pair_hinge_loss, acc_fn=pair_acc, \n",
    "           num_features=10, num_steps=20000, lr=0.0001, keep_prob=0.8, reg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin training\n",
      "\tstep 0: train loss 1.88991, train error 0.93, test loss 1.84976, test error 0.912\n",
      "\tclassification accuracy: 0.5970\n",
      "\tstep 1000: train loss 0.983534, train error 0.44, test loss 0.848343, test error 0.354\n",
      "\tclassification accuracy: 0.8087\n",
      "\tstep 2000: train loss 0.708855, train error 0.25, test loss 0.659009, test error 0.228\n",
      "\tclassification accuracy: 0.8661\n",
      "\tstep 3000: train loss 0.443177, train error 0.15, test loss 0.475632, test error 0.156\n",
      "\tclassification accuracy: 0.8837\n",
      "\tstep 4000: train loss 0.354823, train error 0.09, test loss 0.30488, test error 0.112\n",
      "\tclassification accuracy: 0.9022\n",
      "\tstep 5000: train loss 0.228264, train error 0.08, test loss 0.331379, test error 0.092\n",
      "\tclassification accuracy: 0.9243\n",
      "\tstep 6000: train loss 0.241761, train error 0.07, test loss 0.268142, test error 0.086\n",
      "\tclassification accuracy: 0.9345\n",
      "\tstep 7000: train loss 0.279377, train error 0.08, test loss 0.307102, test error 0.068\n",
      "\tclassification accuracy: 0.9404\n",
      "\tstep 8000: train loss 0.390338, train error 0.12, test loss 0.207852, test error 0.076\n",
      "\tclassification accuracy: 0.9495\n",
      "\tstep 9000: train loss 0.13482, train error 0.04, test loss 0.199016, test error 0.06\n",
      "\tclassification accuracy: 0.9550\n",
      "\tstep 10000: train loss 0.0865624, train error 0.02, test loss 0.117967, test error 0.048\n",
      "\tclassification accuracy: 0.9601\n",
      "\tstep 11000: train loss 0.171261, train error 0.07, test loss 0.098997, test error 0.026\n",
      "\tclassification accuracy: 0.9633\n",
      "\tstep 12000: train loss 0.0200463, train error 0, test loss 0.222424, test error 0.032\n",
      "\tclassification accuracy: 0.9676\n",
      "\tstep 13000: train loss 0.113095, train error 0.03, test loss 0.076598, test error 0.026\n",
      "\tclassification accuracy: 0.9712\n",
      "\tstep 14000: train loss 0.106263, train error 0.03, test loss 0.128074, test error 0.00999999\n",
      "\tclassification accuracy: 0.9736\n",
      "\tstep 15000: train loss 0.0533796, train error 0.02, test loss 0.0477331, test error 0.018\n",
      "\tclassification accuracy: 0.9732\n",
      "\tstep 16000: train loss 0.0817416, train error 0.02, test loss 0.0819333, test error 0.018\n",
      "\tclassification accuracy: 0.9744\n",
      "\tstep 17000: train loss 0.0680276, train error 0.00999999, test loss 0.071195, test error 0.016\n",
      "\tclassification accuracy: 0.9764\n",
      "\tstep 18000: train loss 0.0548031, train error 0.00999999, test loss 0.0802196, test error 0.016\n",
      "\tclassification accuracy: 0.9775\n",
      "\tstep 19000: train loss 0.0624023, train error 0.02, test loss 0.0419366, test error 0.00999999\n",
      "\tclassification accuracy: 0.9777\n",
      "end training // time elapsed: 492.2027 s\n",
      "test set error: 0.0240 // time elapsed: 0.0381 s\n",
      "classification accuracy: 0.9783\n"
     ]
    }
   ],
   "source": [
    "test_model(model=ConvModelSmall, loss_fn=pair_hinge_loss, acc_fn=pair_acc, \n",
    "           num_features=20, num_steps=20000, lr=0.0001, keep_prob=0.8, reg=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConvModel experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization is harming accuracy and loss. Probably using too much before. \n",
    "20 features:\n",
    "    0.5 dropout: 79.56% classification accuracy and 7.80% similarity error\n",
    "    0.8 dropout: 98.83% classification accuracy and 0.4% similarily error\n",
    "10 features: \n",
    "    0.8 dropout: 90.48% classification accuracy and 3.0% similarity error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin training\n",
      "\tstep 0: train loss 1.43683, train error 0.91, test loss 1.42038, test error 0.896\n",
      "\tclassification accuracy: 0.5842\n",
      "\tstep 1000: train loss 0.773272, train error 0.3, test loss 0.830149, test error 0.304\n",
      "\tclassification accuracy: 0.7427\n",
      "\tstep 2000: train loss 0.448876, train error 0.14, test loss 0.50523, test error 0.184\n",
      "\tclassification accuracy: 0.8478\n",
      "\tstep 3000: train loss 0.284996, train error 0.1, test loss 0.426906, test error 0.122\n",
      "\tclassification accuracy: 0.8194\n",
      "\tstep 4000: train loss 0.471221, train error 0.19, test loss 0.356038, test error 0.126\n",
      "\tclassification accuracy: 0.8103\n",
      "\tstep 5000: train loss 0.267139, train error 0.1, test loss 0.274423, test error 0.094\n",
      "\tclassification accuracy: 0.7827\n",
      "\tstep 6000: train loss 0.282087, train error 0.11, test loss 0.27891, test error 0.104\n",
      "\tclassification accuracy: 0.8253\n",
      "\tstep 7000: train loss 0.245154, train error 0.1, test loss 0.279555, test error 0.1\n",
      "\tclassification accuracy: 0.8371\n",
      "\tstep 8000: train loss 0.203949, train error 0.09, test loss 0.261744, test error 0.1\n",
      "\tclassification accuracy: 0.8361\n",
      "\tstep 9000: train loss 0.262195, train error 0.1, test loss 0.21386, test error 0.074\n",
      "\tclassification accuracy: 0.8371\n",
      "\tstep 10000: train loss 0.212468, train error 0.1, test loss 0.25645, test error 0.088\n",
      "\tclassification accuracy: 0.8396\n",
      "\tstep 11000: train loss 0.288538, train error 0.14, test loss 0.206743, test error 0.082\n",
      "\tclassification accuracy: 0.8511\n",
      "\tstep 12000: train loss 0.142835, train error 0.06, test loss 0.13803, test error 0.068\n",
      "\tclassification accuracy: 0.8607\n",
      "\tstep 13000: train loss 0.231165, train error 0.09, test loss 0.154801, test error 0.076\n",
      "\tclassification accuracy: 0.8580\n",
      "\tstep 14000: train loss 0.136166, train error 0.06, test loss 0.198866, test error 0.072\n",
      "\tclassification accuracy: 0.8974\n",
      "\tstep 15000: train loss 0.145742, train error 0.06, test loss 0.222579, test error 0.076\n",
      "\tclassification accuracy: 0.8764\n",
      "\tstep 16000: train loss 0.122326, train error 0.06, test loss 0.198203, test error 0.064\n",
      "\tclassification accuracy: 0.8869\n",
      "\tstep 17000: train loss 0.141079, train error 0.07, test loss 0.117373, test error 0.036\n",
      "\tclassification accuracy: 0.8958\n",
      "\tstep 18000: train loss 0.299426, train error 0.11, test loss 0.170512, test error 0.094\n",
      "\tclassification accuracy: 0.8951\n",
      "\tstep 19000: train loss 0.089034, train error 0.04, test loss 0.124467, test error 0.04\n",
      "\tclassification accuracy: 0.9292\n",
      "end training // time elapsed: 3684.0974 s\n",
      "test set error: 0.0300 // time elapsed: 0.1489 s\n",
      "\tclassification accuracy: 0.9048\n"
     ]
    }
   ],
   "source": [
    "test_model(model=ConvModel, loss_fn=pair_hinge_loss, acc_fn=pair_acc, \n",
    "           num_features=10, num_steps=20000, lr=0.0001, keep_prob=0.8, reg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin training\n",
      "\tstep 0: train loss 1.53103, train error 0.93, test loss 1.50622, test error 0.912\n",
      "\tclassification accuracy: 0.7057\n",
      "\tstep 1000: train loss 0.853747, train error 0.44, test loss 0.860823, test error 0.392\n",
      "\tclassification accuracy: 0.8366\n",
      "\tstep 2000: train loss 0.686833, train error 0.16, test loss 0.514372, test error 0.182\n",
      "\tclassification accuracy: 0.8904\n",
      "\tstep 3000: train loss 0.38264, train error 0.14, test loss 0.339215, test error 0.114\n",
      "\tclassification accuracy: 0.9215\n",
      "\tstep 4000: train loss 0.469941, train error 0.17, test loss 0.417657, test error 0.094\n",
      "\tclassification accuracy: 0.9448\n",
      "\tstep 5000: train loss 0.184857, train error 0.07, test loss 0.244433, test error 0.052\n",
      "\tclassification accuracy: 0.9541\n",
      "\tstep 6000: train loss 0.130037, train error 0.05, test loss 0.145431, test error 0.062\n",
      "\tclassification accuracy: 0.9670\n",
      "\tstep 7000: train loss 0.0942735, train error 0.02, test loss 0.0868136, test error 0.026\n",
      "\tclassification accuracy: 0.9717\n",
      "\tstep 8000: train loss 0.0509903, train error 0.00999999, test loss 0.0855801, test error 0.024\n",
      "\tclassification accuracy: 0.9781\n",
      "\tstep 9000: train loss 0.0779176, train error 0.02, test loss 0.047044, test error 0.016\n",
      "\tclassification accuracy: 0.9792\n",
      "\tstep 10000: train loss 0.03617, train error 0.00999999, test loss 0.0889754, test error 0.022\n",
      "\tclassification accuracy: 0.9821\n",
      "\tstep 11000: train loss 0.0364776, train error 0.00999999, test loss 0.0599092, test error 0.018\n",
      "\tclassification accuracy: 0.9810\n",
      "\tstep 12000: train loss 0.035716, train error 0.00999999, test loss 0.0587171, test error 0.00800002\n",
      "\tclassification accuracy: 0.9864\n",
      "\tstep 13000: train loss 0.0516594, train error 0.00999999, test loss 0.130402, test error 0.018\n",
      "\tclassification accuracy: 0.9859\n",
      "\tstep 14000: train loss 0.0471973, train error 0, test loss 0.0298509, test error 0.00999999\n",
      "\tclassification accuracy: 0.9842\n",
      "\tstep 15000: train loss 0.0271679, train error 0.00999999, test loss 0.0249231, test error 0.00199997\n",
      "\tclassification accuracy: 0.9871\n",
      "\tstep 16000: train loss 0.104733, train error 0.02, test loss 0.0531707, test error 0.016\n",
      "\tclassification accuracy: 0.9865\n",
      "\tstep 17000: train loss 0.00453232, train error 0, test loss 0.0374537, test error 0.016\n",
      "\tclassification accuracy: 0.9871\n",
      "\tstep 18000: train loss 0.0212787, train error 0, test loss 0.0147497, test error 0.00599998\n",
      "\tclassification accuracy: 0.9889\n",
      "\tstep 19000: train loss 0.0402708, train error 0.02, test loss 0.0335043, test error 0.012\n",
      "\tclassification accuracy: 0.9889\n",
      "end training // time elapsed: 2730.5455 s\n",
      "test set error: 0.0040 // time elapsed: 0.1523 s\n",
      "\tclassification accuracy: 0.9883\n"
     ]
    }
   ],
   "source": [
    "test_model(model=ConvModel, loss_fn=pair_hinge_loss, acc_fn=pair_acc, \n",
    "           num_features=20, num_steps=20000, lr=0.0001, keep_prob=0.8, reg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin training\n",
      "\tstep 0: train loss 1.72427, train error 0.91, test loss 1.65946, test error 0.872\n",
      "\tclassification accuracy: 0.7294\n",
      "\tstep 1000: train loss 1.26688, train error 0.86, test loss 1.30334, test error 0.908\n",
      "\tclassification accuracy: 0.6746\n",
      "\tstep 2000: train loss 1.35572, train error 0.89, test loss 1.35587, test error 0.88\n",
      "\tclassification accuracy: 0.7566\n",
      "\tstep 3000: train loss 1.02514, train error 0.47, test loss 1.00683, test error 0.488\n",
      "\tclassification accuracy: 0.8013\n",
      "\tstep 4000: train loss 0.626289, train error 0.19, test loss 0.715968, test error 0.238\n",
      "\tclassification accuracy: 0.7764\n",
      "\tstep 5000: train loss 0.605368, train error 0.19, test loss 0.491022, test error 0.148\n",
      "\tclassification accuracy: 0.7822\n",
      "\tstep 6000: train loss 0.313229, train error 0.1, test loss 0.376109, test error 0.136\n",
      "\tclassification accuracy: 0.8110\n",
      "\tstep 7000: train loss 0.327111, train error 0.08, test loss 0.365875, test error 0.126\n",
      "\tclassification accuracy: 0.8133\n",
      "\tstep 8000: train loss 0.439226, train error 0.11, test loss 0.341567, test error 0.106\n",
      "\tclassification accuracy: 0.8028\n",
      "\tstep 9000: train loss 0.355164, train error 0.13, test loss 0.359042, test error 0.11\n",
      "\tclassification accuracy: 0.7945\n",
      "\tstep 10000: train loss 0.213351, train error 0.00999999, test loss 0.373651, test error 0.108\n",
      "\tclassification accuracy: 0.7971\n",
      "\tstep 11000: train loss 0.175181, train error 0.08, test loss 0.335914, test error 0.104\n",
      "\tclassification accuracy: 0.7887\n",
      "\tstep 12000: train loss 0.279994, train error 0.04, test loss 0.220516, test error 0.066\n",
      "\tclassification accuracy: 0.8079\n",
      "\tstep 13000: train loss 0.17913, train error 0.07, test loss 0.233845, test error 0.074\n",
      "\tclassification accuracy: 0.7938\n",
      "\tstep 14000: train loss 0.235488, train error 0.07, test loss 0.273092, test error 0.06\n",
      "\tclassification accuracy: 0.7940\n",
      "\tstep 15000: train loss 0.243841, train error 0.07, test loss 0.281573, test error 0.088\n",
      "\tclassification accuracy: 0.7884\n",
      "\tstep 16000: train loss 0.264744, train error 0.09, test loss 0.242633, test error 0.066\n",
      "\tclassification accuracy: 0.7992\n",
      "\tstep 17000: train loss 0.190688, train error 0.07, test loss 0.209432, test error 0.064\n",
      "\tclassification accuracy: 0.8010\n",
      "\tstep 18000: train loss 0.200905, train error 0.06, test loss 0.1951, test error 0.066\n",
      "\tclassification accuracy: 0.7996\n",
      "\tstep 19000: train loss 0.236026, train error 0.06, test loss 0.267605, test error 0.078\n",
      "\tclassification accuracy: 0.7957\n",
      "end training // time elapsed: 3344.1969 s\n",
      "test set error: 0.0780 // time elapsed: 0.1173 s\n",
      "\tclassification accuracy: 0.7956\n"
     ]
    }
   ],
   "source": [
    "test_model(model=ConvModel, loss_fn=pair_hinge_loss, acc_fn=pair_acc, \n",
    "           num_features=20, num_steps=20000, lr=0.0001, keep_prob=0.5, reg=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
